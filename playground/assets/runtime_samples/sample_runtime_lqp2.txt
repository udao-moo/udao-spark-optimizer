{"RequestType":"RuntimeQS","TemplateId":"5","QsOptId":5,"QSPhysical":{"operators":{"4":{"sign":-1050047436,"className":"org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec","sizeInBytes":1048584,"rowCount":1,"isRuntime":true,"predicate":" (unknown) BroadcastQueryStage Output [1]: [r_regionkey#45L] Arguments: 3 "},"1":{"sign":-1866602773,"className":"org.apache.spark.sql.execution.joins.BroadcastHashJoinExec","sizeInBytes":1300,"rowCount":25,"isRuntime":false,"predicate":" (unknown) BroadcastHashJoin Left keys [1]: [n_regionkey#43L] Right keys [1]: [r_regionkey#45L] Join type: Inner Join condition: None "},"0":{"sign":-803863342,"className":"org.apache.spark.sql.execution.ProjectExec","sizeInBytes":900,"rowCount":25,"isRuntime":false,"predicate":" (unknown) Project Output [2]: [n_nationkey#41L, n_name#42] Input [4]: [n_nationkey#41L, n_name#42, n_regionkey#43L, r_regionkey#45L] "},"2":{"sign":272472926,"className":"org.apache.spark.sql.execution.FilterExec","sizeInBytes":1100,"rowCount":25,"isRuntime":false,"predicate":" (unknown) Filter Input [3]: [n_nationkey#41L, n_name#42, n_regionkey#43L] Condition : (isnotnull(n_nationkey#41L) AND isnotnull(n_regionkey#43L)) "},"3":{"sign":-2093736135,"className":"org.apache.spark.sql.execution.FileSourceScanExec","sizeInBytes":1100,"rowCount":25,"isRuntime":false,"predicate":" (unknown) Scan parquet spark_catalog.tpch_100.nation Output [3]: [n_nationkey#41L, n_name#42, n_regionkey#43L] Batched: true Location: InMemoryFileIndex [hdfs://node1-opa:8020/user/spark_benchmark/tpch_100/dataset/nation] PushedFilters: [IsNotNull(n_nationkey), IsNotNull(n_regionkey)] ReadSchema: struct<n_nationkey:bigint,n_name:string,n_regionkey:bigint> "}},"links":[{"fromId":3,"fromName":"Scan parquet spark_catalog.tpch_100.nation","toId":2,"toName":"Filter","linkType":"Operator"},{"fromId":2,"fromName":"Filter","toId":1,"toName":"BroadcastHashJoin","linkType":"Operator"},{"fromId":4,"fromName":"BroadcastQueryStage","toId":1,"toName":"BroadcastHashJoin","linkType":"Operator"},{"fromId":1,"fromName":"BroadcastHashJoin","toId":0,"toName":"Project","linkType":"Operator"}],"rawPlan":"Project [n_nationkey#41L, n_name#42]\n+- BroadcastHashJoin [n_regionkey#43L], [r_regionkey#45L], Inner,BuildRight, false\n   :- Filter (isnotnull(n_nationkey#41L) AND isnotnull(n_regionkey#43L))\n   :  +- FileScan parquet spark_catalog.tpch_100.nation[n_nationkey#41L,n_name#42,n_regionkey#43L] Batched: true, DataFilters: [isnotnull(n_nationkey#41L), isnotnull(n_regionkey#43L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node1-opa:8020/user/spark_benchmark/tpch_100/dataset/nation], PartitionFilters: [], PushedFilters: [IsNotNull(n_nationkey), IsNotNull(n_regionkey)], ReadSchema: struct<n_nationkey:bigint,n_name:string,n_regionkey:bigint>\n   +- BroadcastQueryStage 3\n      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=306]\n         +- *(4) Project [r_regionkey#45L]\n            +- *(4) Filter ((isnotnull(r_name#46) AND (r_name#46 = AMERICA)) AND isnotnull(r_regionkey#45L))\n              +- *(4) ColumnarToRow\n                  +- FileScan parquet spark_catalog.tpch_100.region[r_regionkey#45L,r_name#46] Batched: true, DataFilters: [isnotnull(r_name#46), (r_name#46 = AMERICA), isnotnull(r_regionkey#45L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node1-opa:8020/user/spark_benchmark/tpch_100/dataset/region], PartitionFilters: [], PushedFilters: [IsNotNull(r_name), EqualTo(r_name,AMERICA), IsNotNull(r_regionkey)], ReadSchema: struct<r_regionkey:bigint,r_name:string>\n"},"IM":{"inputSizeInBytes":1049684,"inputRowCount":26},"InitialPartitionNum":0,"PD":{},"RunningQueryStageSnapshot":{"RunningTasksNum":20,"FinishedTasksNum":0,"FinishedTasksTotalTimeInMs":0.0,"FinishedTasksDistributionInMs":[0.0,0.0,0.0,0.0,0.0]},"Configuration":{"theta_c":[{"spark.executor.cores":"1"},{"spark.executor.memory":"1g"},{"spark.executor.instances":"4"},{"spark.default.parallelism":"40"},{"spark.reducer.maxSizeInFlight":"48m"},{"spark.shuffle.sort.bypassMergeThreshold":"200"},{"spark.shuffle.compress":"true"},{"spark.memory.fraction":"0.6"}],"theta_p":[{"spark.sql.adaptive.advisoryPartitionSizeInBytes":"16MB"},{"spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin":"0.4"},{"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold":"70MB"},{"spark.sql.adaptive.autoBroadcastJoinThreshold":"180MB"},{"spark.sql.shuffle.partitions":"88"},{"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes":"64MB"},{"spark.sql.adaptive.skewJoin.skewedPartitionFactor":"2"},{"spark.sql.files.maxPartitionBytes":"32MB"},{"spark.sql.files.openCostInBytes":"1MB"}],"theta_s":[{"spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor":"0.05"},{"spark.sql.adaptive.coalescePartitions.minPartitionSize":"2560KB"}]}}
