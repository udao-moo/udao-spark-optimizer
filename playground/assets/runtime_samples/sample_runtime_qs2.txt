{"RequestType":"RuntimeQS","TemplateId":"5","QsOptId":6,"QSPhysical":{"operators":{"0":{"sign":-963935839,"className":"org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec","sizeInBytes":24000000,"rowCount":1000000,"isRuntime":true,"predicate":" (unknown) ShuffleQueryStage Output [2]: [s_suppkey#34L, s_nationkey#37L] Arguments: 1 "}},"links":[],"rawPlan":"ShuffleQueryStage 1\n+- Exchange hashpartitioning(s_suppkey#34L, 200), ENSURE_REQUIREMENTS, [plan_id=260]\n   +- *(2) Filter (isnotnull(s_suppkey#34L) AND isnotnull(s_nationkey#37L))\n      +- *(2) ColumnarToRow\n         +- FileScan parquet spark_catalog.tpch_100.supplier[s_suppkey#34L,s_nationkey#37L] Batched: true, DataFilters: [isnotnull(s_suppkey#34L), isnotnull(s_nationkey#37L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node1-opa:8020/user/spark_benchmark/tpch_100/dataset/supplier], PartitionFilters: [], PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)], ReadSchema: struct<s_suppkey:bigint,s_nationkey:bigint>\n"},"IM":{"inputSizeInBytes":24000000,"inputRowCount":1000000},"InitialPartitionNum":200,"PD":{"1":[52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,52331,57565,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,52331,57565,57565,52331,57565,57565,57565,57565,52331,52331,57565,52331,57565,57565,57565,57565,52331,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,52331,52331,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,52331,57565,57565,52331,52331,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565,52331,57565,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,52331,57565,52331,57565,57565,57565,57565,57565,57565,57565,57565,57565,57565,52331,57565,57565,57565,57565]},"RunningQueryStageSnapshot":{"RunningTasksNum":20,"FinishedTasksNum":23,"FinishedTasksTotalTimeInMs":37735.0,"FinishedTasksDistributionInMs":[929.0,1345.0,1477.0,1945.0,2813.0]},"Configuration":{"theta_c":[{"spark.executor.cores":"5"},{"spark.executor.memory":"16g"},{"spark.executor.instances":"4"},{"spark.default.parallelism":"40"},{"spark.reducer.maxSizeInFlight":"48m"},{"spark.shuffle.sort.bypassMergeThreshold":"200"},{"spark.shuffle.compress":"true"},{"spark.memory.fraction":"0.6"}],"theta_p":[{"spark.sql.adaptive.advisoryPartitionSizeInBytes":"16MB"},{"spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin":"0.2"},{"spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold":"70MB"},{"spark.sql.adaptive.autoBroadcastJoinThreshold":"310MB"},{"spark.sql.shuffle.partitions":"272"},{"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes":"128MB"},{"spark.sql.adaptive.skewJoin.skewedPartitionFactor":"5.2"},{"spark.sql.files.maxPartitionBytes":"32MB"},{"spark.sql.files.openCostInBytes":"1MB"}],"theta_s":[{"spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor":"0.17"},{"spark.sql.adaptive.coalescePartitions.minPartitionSize":"512KB"}]}}
