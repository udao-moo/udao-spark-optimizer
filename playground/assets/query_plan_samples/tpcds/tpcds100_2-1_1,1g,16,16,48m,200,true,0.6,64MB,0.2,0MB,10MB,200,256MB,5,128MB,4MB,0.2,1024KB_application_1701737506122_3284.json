{
  "CompileTimeLQP" : {
    "LQP" : {
      "operators" : {
        "12" : {
          "sign" : -1005230110,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 2865460340,
          "rowCount" : 143273017,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [cs_sold_date_sk#94 AS sold_date_sk#18, cs_ext_sales_price#83 AS sales_price#19] "
        },
        "8" : {
          "sign" : -1644376189,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Union",
          "sizeInBytes" : 4305132820,
          "rowCount" : 215256641,
          "isRuntime" : false,
          "predicate" : " (unknown) Union Arguments: false, false "
        },
        "19" : {
          "sign" : 211617584,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 6912,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_week_seq#127] "
        },
        "23" : {
          "sign" : -440231572,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
          "sizeInBytes" : 41472,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#155 = d_week_seq#99) "
        },
        "4" : {
          "sign" : -2118128056,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
          "sizeInBytes" : 41472,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#127 = d_week_seq#99) "
        },
        "15" : {
          "sign" : 666271073,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 2629764,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_date_sk#95, d_week_seq#99, d_day_name#109] "
        },
        "11" : {
          "sign" : -966747353,
          "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
          "sizeInBytes" : 14972593792,
          "rowCount" : 71983624,
          "isRuntime" : false,
          "predicate" : " (unknown) LogicalRelation Arguments: parquet, [ws_sold_time_sk#27, ws_ship_date_sk#28, ws_item_sk#29, ws_bill_customer_sk#30, ws_bill_cdemo_sk#31, ws_bill_hdemo_sk#32, ws_bill_addr_sk#33, ws_ship_customer_sk#34, ws_ship_cdemo_sk#35, ws_ship_hdemo_sk#36, ws_ship_addr_sk#37, ws_web_page_sk#38, ws_web_site_sk#39, ws_ship_mode_sk#40, ws_warehouse_sk#41, ws_promo_sk#42, ws_order_number#43L, ws_quantity#44, ws_wholesale_cost#45, ws_list_price#46, ws_sales_price#47, ws_ext_discount_amt#48, ws_ext_sales_price#49, ws_ext_wholesale_cost#50, ... 10 more fields], `spark_catalog`.`tpcds_100`.`web_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
        },
        "9" : {
          "sign" : -1879825111,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 1439672480,
          "rowCount" : 71983624,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [ws_sold_date_sk#60 AS sold_date_sk#16, ws_ext_sales_price#49 AS sales_price#17] "
        },
        "22" : {
          "sign" : 1347858420,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 39168,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#8, sun_sales#20 AS sun_sales2#9, mon_sales#21 AS mon_sales2#10, tue_sales#22 AS tue_sales2#11, wed_sales#23 AS wed_sales2#12, thu_sales#24 AS thu_sales2#13, fri_sales#25 AS fri_sales2#14, sat_sales#26 AS sat_sales2#15] "
        },
        "26" : {
          "sign" : 1328046988,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
          "sizeInBytes" : 754765553683632,
          "rowCount" : 15724282368409,
          "isRuntime" : false,
          "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#16) "
        },
        "13" : {
          "sign" : -530295932,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
          "sizeInBytes" : 29800787536,
          "rowCount" : 143273017,
          "isRuntime" : false,
          "predicate" : " (unknown) Filter Arguments: isnotnull(cs_sold_date_sk#94) "
        },
        "24" : {
          "sign" : -1699010041,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
          "sizeInBytes" : 680680,
          "rowCount" : 10010,
          "isRuntime" : false,
          "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#17 END)),17,2) AS sun_sales#20, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#17 END)),17,2) AS mon_sales#21, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#17 END)),17,2) AS tue_sales#22, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#17 END)),17,2) AS wed_sales#23, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#17 END)),17,2) AS thu_sales#24, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#17 END)),17,2) AS fri_sales#25, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#17 END)),17,2) AS sat_sales#26] "
        },
        "16" : {
          "sign" : 1282660435,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
          "sizeInBytes" : 17970054,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#208 [], xxhash64(d_week_seq#99, 42))) "
        },
        "5" : {
          "sign" : -1005305993,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
          "sizeInBytes" : 680680,
          "rowCount" : 10010,
          "isRuntime" : false,
          "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#17 END)),17,2) AS sun_sales#20, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#17 END)),17,2) AS mon_sales#21, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#17 END)),17,2) AS tue_sales#22, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#17 END)),17,2) AS wed_sales#23, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#17 END)),17,2) AS thu_sales#24, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#17 END)),17,2) AS fri_sales#25, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#17 END)),17,2) AS sat_sales#26] "
        },
        "10" : {
          "sign" : 224036230,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
          "sizeInBytes" : 14972593792,
          "rowCount" : 71983624,
          "isRuntime" : false,
          "predicate" : " (unknown) Filter Arguments: isnotnull(ws_sold_date_sk#60) "
        },
        "21" : {
          "sign" : -489954868,
          "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
          "sizeInBytes" : 17970054,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#123, d_date_id#124, d_date#125, d_month_seq#126, d_week_seq#127, d_quarter_seq#128, d_year#129, d_dow#130, d_moy#131, d_dom#132, d_qoy#133, d_fy_year#134, d_fy_quarter_seq#135, d_fy_week_seq#136, d_day_name#137, d_quarter_name#138, d_holiday#139, d_weekend#140, d_following_holiday#141, d_first_dom#142, d_last_dom#143, d_same_day_ly#144, d_same_day_lq#145, d_current_day#146, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
        },
        "32" : {
          "sign" : -910385438,
          "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
          "sizeInBytes" : 17970054,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#151, d_date_id#152, d_date#153, d_month_seq#154, d_week_seq#155, d_quarter_seq#156, d_year#157, d_dow#158, d_moy#159, d_dom#160, d_qoy#161, d_fy_year#162, d_fy_quarter_seq#163, d_fy_week_seq#164, d_day_name#165, d_quarter_name#166, d_holiday#167, d_weekend#168, d_following_holiday#169, d_first_dom#170, d_last_dom#171, d_same_day_ly#172, d_same_day_lq#173, d_current_day#174, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
        },
        "6" : {
          "sign" : -727423926,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 628971294736360,
          "rowCount" : 15724282368409,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [sales_price#17, d_week_seq#99, d_day_name#109] "
        },
        "1" : {
          "sign" : 991456560,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 41140224,
          "rowCount" : 331776,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_week_seq1#0, round((sun_sales1#1 / sun_sales2#9), 2) AS round((sun_sales1 / sun_sales2), 2)#189, round((mon_sales1#2 / mon_sales2#10), 2) AS round((mon_sales1 / mon_sales2), 2)#190, round((tue_sales1#3 / tue_sales2#11), 2) AS round((tue_sales1 / tue_sales2), 2)#191, round((wed_sales1#4 / wed_sales2#12), 2) AS round((wed_sales1 / wed_sales2), 2)#192, round((thu_sales1#5 / thu_sales2#13), 2) AS round((thu_sales1 / thu_sales2), 2)#193, round((fri_sales1#6 / fri_sales2#14), 2) AS round((fri_sales1 / fri_sales2), 2)#194, round((sat_sales1#7 / sat_sales2#15), 2) AS round((sat_sales1 / sat_sales2), 2)#195] "
        },
        "17" : {
          "sign" : -513469665,
          "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
          "sizeInBytes" : 17970054,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#95, d_date_id#96, d_date#97, d_month_seq#98, d_week_seq#99, d_quarter_seq#100, d_year#101, d_dow#102, d_moy#103, d_dom#104, d_qoy#105, d_fy_year#106, d_fy_quarter_seq#107, d_fy_week_seq#108, d_day_name#109, d_quarter_name#110, d_holiday#111, d_weekend#112, d_following_holiday#113, d_first_dom#114, d_last_dom#115, d_same_day_ly#116, d_same_day_lq#117, d_current_day#118, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
        },
        "25" : {
          "sign" : -731252984,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 628971294736360,
          "rowCount" : 15724282368409,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [sales_price#17, d_week_seq#99, d_day_name#109] "
        },
        "14" : {
          "sign" : 1507442072,
          "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
          "sizeInBytes" : 29800787536,
          "rowCount" : 143273017,
          "isRuntime" : false,
          "predicate" : " (unknown) LogicalRelation Arguments: parquet, [cs_sold_time_sk#61, cs_ship_date_sk#62, cs_bill_customer_sk#63, cs_bill_cdemo_sk#64, cs_bill_hdemo_sk#65, cs_bill_addr_sk#66, cs_ship_customer_sk#67, cs_ship_cdemo_sk#68, cs_ship_hdemo_sk#69, cs_ship_addr_sk#70, cs_call_center_sk#71, cs_catalog_page_sk#72, cs_ship_mode_sk#73, cs_warehouse_sk#74, cs_item_sk#75, cs_promo_sk#76, cs_order_number#77L, cs_quantity#78, cs_wholesale_cost#79, cs_list_price#80, cs_sales_price#81, cs_ext_discount_amt#82, cs_ext_sales_price#83, cs_ext_wholesale_cost#84, ... 10 more fields], `spark_catalog`.`tpcds_100`.`catalog_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
        },
        "31" : {
          "sign" : -1866701991,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
          "sizeInBytes" : 141696,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_year#157) AND (d_year#157 = 2002)) AND isnotnull(d_week_seq#155)) "
        },
        "0" : {
          "sign" : 328553008,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
          "sizeInBytes" : 41140224,
          "rowCount" : 331776,
          "isRuntime" : false,
          "predicate" : " (unknown) Sort Arguments: [d_week_seq1#0 ASC NULLS FIRST], true "
        },
        "20" : {
          "sign" : -1111989207,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
          "sizeInBytes" : 141696,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_year#129) AND (d_year#129 = 2001)) AND isnotnull(d_week_seq#127)) "
        },
        "27" : {
          "sign" : -1884399905,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 2629764,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_date_sk#95, d_week_seq#99, d_day_name#109] "
        },
        "2" : {
          "sign" : -892515156,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
          "sizeInBytes" : 42467328,
          "rowCount" : 331776,
          "isRuntime" : false,
          "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#0 = (d_week_seq2#8 - 53)) "
        },
        "18" : {
          "sign" : -651933971,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
          "sizeInBytes" : 108,
          "rowCount" : 1,
          "isRuntime" : false,
          "predicate" : " (unknown) Aggregate Arguments: [bloom_filter_agg(xxhash64(d_week_seq#127, 42), 576, 14808, 0, 0) AS bloomFilter#207] "
        },
        "30" : {
          "sign" : -1394273401,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 6912,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_week_seq#155] "
        },
        "7" : {
          "sign" : -474346564,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
          "sizeInBytes" : 754765553683632,
          "rowCount" : 15724282368409,
          "isRuntime" : false,
          "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#16) "
        },
        "29" : {
          "sign" : 714136149,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
          "sizeInBytes" : 108,
          "rowCount" : 1,
          "isRuntime" : false,
          "predicate" : " (unknown) Aggregate Arguments: [bloom_filter_agg(xxhash64(d_week_seq#155, 42), 576, 14808, 0, 0) AS bloomFilter#210] "
        },
        "3" : {
          "sign" : -972785586,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
          "sizeInBytes" : 39168,
          "rowCount" : 576,
          "isRuntime" : false,
          "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#0, sun_sales#20 AS sun_sales1#1, mon_sales#21 AS mon_sales1#2, tue_sales#22 AS tue_sales1#3, wed_sales#23 AS wed_sales1#4, thu_sales#24 AS thu_sales1#5, fri_sales#25 AS fri_sales1#6, sat_sales#26 AS sat_sales1#7] "
        },
        "28" : {
          "sign" : 301641059,
          "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
          "sizeInBytes" : 17970054,
          "rowCount" : 73049,
          "isRuntime" : false,
          "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#211 [], xxhash64(d_week_seq#99, 42))) "
        }
      },
      "links" : [ {
        "fromId" : 11,
        "fromName" : "LogicalRelation",
        "toId" : 10,
        "toName" : "Filter",
        "linkType" : "Operator"
      }, {
        "fromId" : 10,
        "fromName" : "Filter",
        "toId" : 9,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 9,
        "fromName" : "Project",
        "toId" : 8,
        "toName" : "Union",
        "linkType" : "Operator"
      }, {
        "fromId" : 14,
        "fromName" : "LogicalRelation",
        "toId" : 13,
        "toName" : "Filter",
        "linkType" : "Operator"
      }, {
        "fromId" : 13,
        "fromName" : "Filter",
        "toId" : 12,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 12,
        "fromName" : "Project",
        "toId" : 8,
        "toName" : "Union",
        "linkType" : "Operator"
      }, {
        "fromId" : 8,
        "fromName" : "Union",
        "toId" : 7,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 17,
        "fromName" : "LogicalRelation",
        "toId" : 16,
        "toName" : "Filter",
        "linkType" : "Operator"
      }, {
        "fromId" : 21,
        "fromName" : "LogicalRelation",
        "toId" : 20,
        "toName" : "Filter",
        "linkType" : "Operator"
      }, {
        "fromId" : 20,
        "fromName" : "Filter",
        "toId" : 19,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 19,
        "fromName" : "Project",
        "toId" : 18,
        "toName" : "Aggregate",
        "linkType" : "Operator"
      }, {
        "fromId" : 18,
        "fromName" : "Aggregate",
        "toId" : 16,
        "toName" : "Filter",
        "linkType" : "Subquery"
      }, {
        "fromId" : 16,
        "fromName" : "Filter",
        "toId" : 15,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 15,
        "fromName" : "Project",
        "toId" : 7,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 7,
        "fromName" : "Join",
        "toId" : 6,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 6,
        "fromName" : "Project",
        "toId" : 5,
        "toName" : "Aggregate",
        "linkType" : "Operator"
      }, {
        "fromId" : 5,
        "fromName" : "Aggregate",
        "toId" : 4,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 19,
        "fromName" : "Project",
        "toId" : 4,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 4,
        "fromName" : "Join",
        "toId" : 3,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 3,
        "fromName" : "Project",
        "toId" : 2,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 8,
        "fromName" : "Union",
        "toId" : 26,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 17,
        "fromName" : "LogicalRelation",
        "toId" : 28,
        "toName" : "Filter",
        "linkType" : "Operator"
      }, {
        "fromId" : 32,
        "fromName" : "LogicalRelation",
        "toId" : 31,
        "toName" : "Filter",
        "linkType" : "Operator"
      }, {
        "fromId" : 31,
        "fromName" : "Filter",
        "toId" : 30,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 30,
        "fromName" : "Project",
        "toId" : 29,
        "toName" : "Aggregate",
        "linkType" : "Operator"
      }, {
        "fromId" : 29,
        "fromName" : "Aggregate",
        "toId" : 28,
        "toName" : "Filter",
        "linkType" : "Subquery"
      }, {
        "fromId" : 28,
        "fromName" : "Filter",
        "toId" : 27,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 27,
        "fromName" : "Project",
        "toId" : 26,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 26,
        "fromName" : "Join",
        "toId" : 25,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 25,
        "fromName" : "Project",
        "toId" : 24,
        "toName" : "Aggregate",
        "linkType" : "Operator"
      }, {
        "fromId" : 24,
        "fromName" : "Aggregate",
        "toId" : 23,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 30,
        "fromName" : "Project",
        "toId" : 23,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 23,
        "fromName" : "Join",
        "toId" : 22,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 22,
        "fromName" : "Project",
        "toId" : 2,
        "toName" : "Join",
        "linkType" : "Operator"
      }, {
        "fromId" : 2,
        "fromName" : "Join",
        "toId" : 1,
        "toName" : "Project",
        "linkType" : "Operator"
      }, {
        "fromId" : 1,
        "fromName" : "Project",
        "toId" : 0,
        "toName" : "Sort",
        "linkType" : "Operator"
      } ],
      "rawPlan" : "Sort [d_week_seq1#0 ASC NULLS FIRST], true\n+- Project [d_week_seq1#0, round((sun_sales1#1 / sun_sales2#9), 2) AS round((sun_sales1 / sun_sales2), 2)#189, round((mon_sales1#2 / mon_sales2#10), 2) AS round((mon_sales1 / mon_sales2), 2)#190, round((tue_sales1#3 / tue_sales2#11), 2) AS round((tue_sales1 / tue_sales2), 2)#191, round((wed_sales1#4 / wed_sales2#12), 2) AS round((wed_sales1 / wed_sales2), 2)#192, round((thu_sales1#5 / thu_sales2#13), 2) AS round((thu_sales1 / thu_sales2), 2)#193, round((fri_sales1#6 / fri_sales2#14), 2) AS round((fri_sales1 / fri_sales2), 2)#194, round((sat_sales1#7 / sat_sales2#15), 2) AS round((sat_sales1 / sat_sales2), 2)#195]\n   +- Join Inner, (d_week_seq1#0 = (d_week_seq2#8 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#0, sun_sales#20 AS sun_sales1#1, mon_sales#21 AS mon_sales1#2, tue_sales#22 AS tue_sales1#3, wed_sales#23 AS wed_sales1#4, thu_sales#24 AS thu_sales1#5, fri_sales#25 AS fri_sales1#6, sat_sales#26 AS sat_sales1#7]\n      :  +- Join Inner, (d_week_seq#127 = d_week_seq#99)\n      :     :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#17 END)),17,2) AS sun_sales#20, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#17 END)),17,2) AS mon_sales#21, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#17 END)),17,2) AS tue_sales#22, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#17 END)),17,2) AS wed_sales#23, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#17 END)),17,2) AS thu_sales#24, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#17 END)),17,2) AS fri_sales#25, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#17 END)),17,2) AS sat_sales#26]\n      :     :  +- Project [sales_price#17, d_week_seq#99, d_day_name#109]\n      :     :     +- Join Inner, (d_date_sk#95 = sold_date_sk#16)\n      :     :        :- Union false, false\n      :     :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#16, ws_ext_sales_price#49 AS sales_price#17]\n      :     :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :     :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :     :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#18, cs_ext_sales_price#83 AS sales_price#19]\n      :     :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :     :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      :     :        +- Project [d_date_sk#95, d_week_seq#99, d_day_name#109]\n      :     :           +- Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#208 [], xxhash64(d_week_seq#99, 42)))\n      :     :              :  +- Aggregate [bloom_filter_agg(xxhash64(d_week_seq#127, 42), 576, 14808, 0, 0) AS bloomFilter#207]\n      :     :              :     +- Project [d_week_seq#127]\n      :     :              :        +- Filter ((isnotnull(d_year#129) AND (d_year#129 = 2001)) AND isnotnull(d_week_seq#127))\n      :     :              :           +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#123,d_date_id#124,d_date#125,d_month_seq#126,d_week_seq#127,d_quarter_seq#128,d_year#129,d_dow#130,d_moy#131,d_dom#132,d_qoy#133,d_fy_year#134,d_fy_quarter_seq#135,d_fy_week_seq#136,d_day_name#137,d_quarter_name#138,d_holiday#139,d_weekend#140,d_following_holiday#141,d_first_dom#142,d_last_dom#143,d_same_day_ly#144,d_same_day_lq#145,d_current_day#146,... 4 more fields] parquet\n      :     :              +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_date_id#96,d_date#97,d_month_seq#98,d_week_seq#99,d_quarter_seq#100,d_year#101,d_dow#102,d_moy#103,d_dom#104,d_qoy#105,d_fy_year#106,d_fy_quarter_seq#107,d_fy_week_seq#108,d_day_name#109,d_quarter_name#110,d_holiday#111,d_weekend#112,d_following_holiday#113,d_first_dom#114,d_last_dom#115,d_same_day_ly#116,d_same_day_lq#117,d_current_day#118,... 4 more fields] parquet\n      :     +- Project [d_week_seq#127]\n      :        +- Filter ((isnotnull(d_year#129) AND (d_year#129 = 2001)) AND isnotnull(d_week_seq#127))\n      :           +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#123,d_date_id#124,d_date#125,d_month_seq#126,d_week_seq#127,d_quarter_seq#128,d_year#129,d_dow#130,d_moy#131,d_dom#132,d_qoy#133,d_fy_year#134,d_fy_quarter_seq#135,d_fy_week_seq#136,d_day_name#137,d_quarter_name#138,d_holiday#139,d_weekend#140,d_following_holiday#141,d_first_dom#142,d_last_dom#143,d_same_day_ly#144,d_same_day_lq#145,d_current_day#146,... 4 more fields] parquet\n      +- Project [d_week_seq#99 AS d_week_seq2#8, sun_sales#20 AS sun_sales2#9, mon_sales#21 AS mon_sales2#10, tue_sales#22 AS tue_sales2#11, wed_sales#23 AS wed_sales2#12, thu_sales#24 AS thu_sales2#13, fri_sales#25 AS fri_sales2#14, sat_sales#26 AS sat_sales2#15]\n         +- Join Inner, (d_week_seq#155 = d_week_seq#99)\n            :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#17 END)),17,2) AS sun_sales#20, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#17 END)),17,2) AS mon_sales#21, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#17 END)),17,2) AS tue_sales#22, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#17 END)),17,2) AS wed_sales#23, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#17 END)),17,2) AS thu_sales#24, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#17 END)),17,2) AS fri_sales#25, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#17 END)),17,2) AS sat_sales#26]\n            :  +- Project [sales_price#17, d_week_seq#99, d_day_name#109]\n            :     +- Join Inner, (d_date_sk#95 = sold_date_sk#16)\n            :        :- Union false, false\n            :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#16, ws_ext_sales_price#49 AS sales_price#17]\n            :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n            :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n            :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#18, cs_ext_sales_price#83 AS sales_price#19]\n            :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n            :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n            :        +- Project [d_date_sk#95, d_week_seq#99, d_day_name#109]\n            :           +- Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#211 [], xxhash64(d_week_seq#99, 42)))\n            :              :  +- Aggregate [bloom_filter_agg(xxhash64(d_week_seq#155, 42), 576, 14808, 0, 0) AS bloomFilter#210]\n            :              :     +- Project [d_week_seq#155]\n            :              :        +- Filter ((isnotnull(d_year#157) AND (d_year#157 = 2002)) AND isnotnull(d_week_seq#155))\n            :              :           +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#151,d_date_id#152,d_date#153,d_month_seq#154,d_week_seq#155,d_quarter_seq#156,d_year#157,d_dow#158,d_moy#159,d_dom#160,d_qoy#161,d_fy_year#162,d_fy_quarter_seq#163,d_fy_week_seq#164,d_day_name#165,d_quarter_name#166,d_holiday#167,d_weekend#168,d_following_holiday#169,d_first_dom#170,d_last_dom#171,d_same_day_ly#172,d_same_day_lq#173,d_current_day#174,... 4 more fields] parquet\n            :              +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_date_id#96,d_date#97,d_month_seq#98,d_week_seq#99,d_quarter_seq#100,d_year#101,d_dow#102,d_moy#103,d_dom#104,d_qoy#105,d_fy_year#106,d_fy_quarter_seq#107,d_fy_week_seq#108,d_day_name#109,d_quarter_name#110,d_holiday#111,d_weekend#112,d_following_holiday#113,d_first_dom#114,d_last_dom#115,d_same_day_ly#116,d_same_day_lq#117,d_current_day#118,... 4 more fields] parquet\n            +- Project [d_week_seq#155]\n               +- Filter ((isnotnull(d_year#157) AND (d_year#157 = 2002)) AND isnotnull(d_week_seq#155))\n                  +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#151,d_date_id#152,d_date#153,d_month_seq#154,d_week_seq#155,d_quarter_seq#156,d_year#157,d_dow#158,d_moy#159,d_dom#160,d_qoy#161,d_fy_year#162,d_fy_quarter_seq#163,d_fy_week_seq#164,d_day_name#165,d_quarter_name#166,d_holiday#167,d_weekend#168,d_following_holiday#169,d_first_dom#170,d_last_dom#171,d_same_day_ly#172,d_same_day_lq#173,d_current_day#174,... 4 more fields] parquet\n"
    },
    "IM" : {
      "inputSizeInBytes" : 89618642872,
      "inputRowCount" : 430805478
    },
    "PD" : { },
    "Configuration" : {
      "theta_c" : [ {
        "spark.executor.memory" : "1g"
      }, {
        "spark.executor.cores" : "1"
      }, {
        "spark.executor.instances" : "16"
      }, {
        "spark.default.parallelism" : "16"
      }, {
        "spark.reducer.maxSizeInFlight" : "48m"
      }, {
        "spark.shuffle.sort.bypassMergeThreshold" : "200"
      }, {
        "spark.shuffle.compress" : "true"
      }, {
        "spark.memory.fraction" : "0.6"
      } ],
      "theta_p" : [ {
        "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
      }, {
        "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
      }, {
        "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
      }, {
        "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
      }, {
        "spark.sql.shuffle.partitions" : "200"
      }, {
        "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
      }, {
        "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
      }, {
        "spark.sql.files.maxPartitionBytes" : "128MB"
      }, {
        "spark.sql.files.openCostInBytes" : "4MB"
      } ],
      "theta_s" : [ {
        "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
      }, {
        "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
      } ]
    }
  },
  "RuntimeLQPs" : {
    "8" : {
      "LQP" : {
        "operators" : {
          "4" : {
            "sign" : 566986461,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 16927240,
            "rowCount" : 248930,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "5" : {
            "sign" : -988046795,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 17922960,
            "rowCount" : 248930,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          },
          "6" : {
            "sign" : -1044119462,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 49104,
            "rowCount" : 682,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))]) "
          },
          "1" : {
            "sign" : 315577656,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 11266571800,
            "rowCount" : 90859450,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "0" : {
            "sign" : 531249589,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 11266571800,
            "rowCount" : 90859450,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "2" : {
            "sign" : 301218942,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 11630009600,
            "rowCount" : 90859450,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "7" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "3" : {
            "sign" : 987296074,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221], BroadcastQueryStage 6 "
          }
        },
        "links" : [ {
          "fromId" : 3,
          "fromName" : "LogicalQueryStage",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "LogicalQueryStage",
          "toId" : 5,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "LogicalQueryStage",
          "toId" : 5,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "Join",
          "toId" : 4,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- LogicalQueryStage Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221], BroadcastQueryStage 6\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- LogicalQueryStage Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))])\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "IM" : {
        "inputSizeInBytes" : 2147104,
        "inputRowCount" : 1412
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226518587,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 1042,
        "IOBytes" : {
          "Total" : 119942,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 111174,
            "SW" : 8768
          }
        }
      }
    },
    "4" : {
      "LQP" : {
        "operators" : {
          "12" : {
            "sign" : 956450284,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] "
          },
          "8" : {
            "sign" : 963324291,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Union",
            "sizeInBytes" : 4305132820,
            "rowCount" : 215256641,
            "isRuntime" : false,
            "predicate" : " (unknown) Union Arguments: false, false "
          },
          "19" : {
            "sign" : -1945033734,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "23" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "4" : {
            "sign" : 530772993,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 26280,
            "rowCount" : 365,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#245 = d_week_seq#99) "
          },
          "15" : {
            "sign" : 1586732036,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0 "
          },
          "11" : {
            "sign" : -966747353,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "sizeInBytes" : 14972593792,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [ws_sold_time_sk#27, ws_ship_date_sk#28, ws_item_sk#29, ws_bill_customer_sk#30, ws_bill_cdemo_sk#31, ws_bill_hdemo_sk#32, ws_bill_addr_sk#33, ws_ship_customer_sk#34, ws_ship_cdemo_sk#35, ws_ship_hdemo_sk#36, ws_ship_addr_sk#37, ws_web_page_sk#38, ws_web_site_sk#39, ws_ship_mode_sk#40, ws_warehouse_sk#41, ws_promo_sk#42, ws_order_number#43L, ws_quantity#44, ws_wholesale_cost#45, ws_list_price#46, ws_sales_price#47, ws_ext_discount_amt#48, ws_ext_sales_price#49, ws_ext_wholesale_cost#50, ... 10 more fields], `spark_catalog`.`tpcds_100`.`web_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "9" : {
            "sign" : 487343071,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] "
          },
          "22" : {
            "sign" : 1586732098,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1051544,
            "rowCount" : 371,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2 "
          },
          "13" : {
            "sign" : -530295932,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "sizeInBytes" : 29800787536,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(cs_sold_date_sk#94) "
          },
          "16" : {
            "sign" : -514385727,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#245], BroadcastQueryStage 1 "
          },
          "5" : {
            "sign" : -1945033796,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "sizeInBytes" : 680680,
            "rowCount" : 10010,
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "10" : {
            "sign" : 224036230,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "sizeInBytes" : 14972593792,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(ws_sold_date_sk#60) "
          },
          "21" : {
            "sign" : 1964464157,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 3833290262928,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "6" : {
            "sign" : 1158039841,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 628971294736360,
            "rowCount" : 15724282368409,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "1" : {
            "sign" : 1945054853,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 123318823892972266,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "17" : {
            "sign" : -393680579,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 5128801388806,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "14" : {
            "sign" : 1507442072,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "sizeInBytes" : 29800787536,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [cs_sold_time_sk#61, cs_ship_date_sk#62, cs_bill_customer_sk#63, cs_bill_cdemo_sk#64, cs_bill_hdemo_sk#65, cs_bill_addr_sk#66, cs_ship_customer_sk#67, cs_ship_cdemo_sk#68, cs_ship_hdemo_sk#69, cs_ship_addr_sk#70, cs_call_center_sk#71, cs_catalog_page_sk#72, cs_ship_mode_sk#73, cs_warehouse_sk#74, cs_item_sk#75, cs_promo_sk#76, cs_order_number#77L, cs_quantity#78, cs_wholesale_cost#79, cs_list_price#80, cs_sales_price#81, cs_ext_discount_amt#82, cs_ext_sales_price#83, cs_ext_wholesale_cost#84, ... 10 more fields], `spark_catalog`.`tpcds_100`.`catalog_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "0" : {
            "sign" : 971901188,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 123318823892972266,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "20" : {
            "sign" : 1158039903,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 3194408552440,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "2" : {
            "sign" : -230302309,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 127296850470164920,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "18" : {
            "sign" : 1341645003,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 5430495588148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          },
          "7" : {
            "sign" : 1964464095,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 754765553683632,
            "rowCount" : 15724282368409,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "3" : {
            "sign" : 145894295,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 24820,
            "rowCount" : 365,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] "
          }
        },
        "links" : [ {
          "fromId" : 11,
          "fromName" : "LogicalRelation",
          "toId" : 10,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 10,
          "fromName" : "Filter",
          "toId" : 9,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "Project",
          "toId" : 8,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 14,
          "fromName" : "LogicalRelation",
          "toId" : 13,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 13,
          "fromName" : "Filter",
          "toId" : 12,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 12,
          "fromName" : "Project",
          "toId" : 8,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Union",
          "toId" : 7,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 15,
          "fromName" : "LogicalQueryStage",
          "toId" : 7,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Join",
          "toId" : 6,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "Project",
          "toId" : 5,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "Aggregate",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 16,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Join",
          "toId" : 3,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Union",
          "toId" : 21,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 22,
          "fromName" : "LogicalQueryStage",
          "toId" : 21,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 21,
          "fromName" : "Join",
          "toId" : 20,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 20,
          "fromName" : "Project",
          "toId" : 19,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 19,
          "fromName" : "Aggregate",
          "toId" : 18,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 23,
          "fromName" : "LogicalQueryStage",
          "toId" : 18,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 18,
          "fromName" : "Join",
          "toId" : 17,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 17,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n      :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n      :     :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n      :     :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n      :     :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n      :     :        :- Union false, false\n      :     :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :     :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :     :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :     :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :     :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      :     :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n      :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n            :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n            :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n            :        :- Union false, false\n            :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n            :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n            :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n            :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n            :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n            :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n            :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "IM" : {
        "inputSizeInBytes" : 89552541964,
        "inputRowCount" : 430587432
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 1,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226503374,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 16255,
        "IOBytes" : {
          "Total" : 1831056775,
          "Details" : {
            "IR" : 1830781810,
            "IW" : 0,
            "SR" : 163829,
            "SW" : 111136
          }
        }
      }
    },
    "9" : {
      "LQP" : {
        "operators" : {
          "0" : {
            "sign" : -1103121560,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 462392,
            "rowCount" : 2513,
            "isRuntime" : true,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "1" : {
            "sign" : -1333415949,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 462392,
            "rowCount" : 2513,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310], ShuffleQueryStage 7 "
          }
        },
        "links" : [ {
          "fromId" : 1,
          "fromName" : "LogicalQueryStage",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- LogicalQueryStage Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310], ShuffleQueryStage 7\n"
      },
      "IM" : {
        "inputSizeInBytes" : 462392,
        "inputRowCount" : 2513
      },
      "PD" : {
        "4" : [ 304, 171, 171, 171, 189, 171, 189, 189, 189, 171, 189, 171, 189, 171, 171, 189, 171, 171, 171, 189, 171, 171, 171, 171, 171, 171, 171, 171, 171, 189, 189, 171, 171, 171, 171, 171, 171, 171, 171, 171, 171, 171, 189, 171, 171, 171, 171, 171, 171, 171, 156, 156, 0 ]
      },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226519318,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 311,
        "IOBytes" : {
          "Total" : 8768,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 8768,
            "SW" : 0
          }
        }
      }
    },
    "5" : {
      "LQP" : {
        "operators" : {
          "12" : {
            "sign" : 956450284,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] "
          },
          "8" : {
            "sign" : 963324291,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Union",
            "sizeInBytes" : 4305132820,
            "rowCount" : 215256641,
            "isRuntime" : false,
            "predicate" : " (unknown) Union Arguments: false, false "
          },
          "19" : {
            "sign" : -1044119462,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))]) "
          },
          "4" : {
            "sign" : 530772993,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 5430495588148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#245 = d_week_seq#99) "
          },
          "15" : {
            "sign" : 1586732036,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1051544,
            "rowCount" : 371,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0 "
          },
          "11" : {
            "sign" : -966747353,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "sizeInBytes" : 14972593792,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [ws_sold_time_sk#27, ws_ship_date_sk#28, ws_item_sk#29, ws_bill_customer_sk#30, ws_bill_cdemo_sk#31, ws_bill_hdemo_sk#32, ws_bill_addr_sk#33, ws_ship_customer_sk#34, ws_ship_cdemo_sk#35, ws_ship_hdemo_sk#36, ws_ship_addr_sk#37, ws_web_page_sk#38, ws_web_site_sk#39, ws_ship_mode_sk#40, ws_warehouse_sk#41, ws_promo_sk#42, ws_order_number#43L, ws_quantity#44, ws_wholesale_cost#45, ws_list_price#46, ws_sales_price#47, ws_ext_discount_amt#48, ws_ext_sales_price#49, ws_ext_wholesale_cost#50, ... 10 more fields], `spark_catalog`.`tpcds_100`.`web_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "9" : {
            "sign" : 487343071,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] "
          },
          "13" : {
            "sign" : -530295932,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "sizeInBytes" : 29800787536,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(cs_sold_date_sk#94) "
          },
          "16" : {
            "sign" : -514385727,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#245], BroadcastQueryStage 1 "
          },
          "5" : {
            "sign" : -1945033796,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "10" : {
            "sign" : 224036230,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "sizeInBytes" : 14972593792,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(ws_sold_date_sk#60) "
          },
          "6" : {
            "sign" : 1158039841,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 3194408552440,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "1" : {
            "sign" : 1106746469,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 26731226313221531521564212843607,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "17" : {
            "sign" : 566986461,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 5380111617590349111,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "14" : {
            "sign" : 1507442072,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "sizeInBytes" : 29800787536,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [cs_sold_time_sk#61, cs_ship_date_sk#62, cs_bill_customer_sk#63, cs_bill_cdemo_sk#64, cs_bill_hdemo_sk#65, cs_bill_addr_sk#66, cs_ship_customer_sk#67, cs_ship_cdemo_sk#68, cs_ship_hdemo_sk#69, cs_ship_addr_sk#70, cs_call_center_sk#71, cs_catalog_page_sk#72, cs_ship_mode_sk#73, cs_warehouse_sk#74, cs_item_sk#75, cs_promo_sk#76, cs_order_number#77L, cs_quantity#78, cs_wholesale_cost#79, cs_list_price#80, cs_sales_price#81, cs_ext_discount_amt#82, cs_ext_sales_price#83, cs_ext_wholesale_cost#84, ... 10 more fields], `spark_catalog`.`tpcds_100`.`catalog_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "0" : {
            "sign" : 1761219822,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 26731226313221531521564212843607,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "20" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "2" : {
            "sign" : 841849541,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 27593523936228677699679187451466,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "18" : {
            "sign" : -988046795,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 5696588771566252000,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          },
          "7" : {
            "sign" : 1964464095,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 3833290262928,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "3" : {
            "sign" : 145894295,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 5128801388806,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] "
          }
        },
        "links" : [ {
          "fromId" : 11,
          "fromName" : "LogicalRelation",
          "toId" : 10,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 10,
          "fromName" : "Filter",
          "toId" : 9,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "Project",
          "toId" : 8,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 14,
          "fromName" : "LogicalRelation",
          "toId" : 13,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 13,
          "fromName" : "Filter",
          "toId" : 12,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 12,
          "fromName" : "Project",
          "toId" : 8,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Union",
          "toId" : 7,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 15,
          "fromName" : "LogicalQueryStage",
          "toId" : 7,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Join",
          "toId" : 6,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "Project",
          "toId" : 5,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "Aggregate",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 16,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Join",
          "toId" : 3,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 19,
          "fromName" : "LogicalQueryStage",
          "toId" : 18,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 20,
          "fromName" : "LogicalQueryStage",
          "toId" : 18,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 18,
          "fromName" : "Join",
          "toId" : 17,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 17,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n      :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n      :     :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n      :     :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n      :     :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n      :     :        :- Union false, false\n      :     :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :     :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :     :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :     :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :     :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      :     :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n      :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- LogicalQueryStage Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))])\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "IM" : {
        "inputSizeInBytes" : 5475271070020,
        "inputRowCount" : 215257742
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 11,
        "FinishedTasksNum" : 224,
        "FinishedTasksTotalTimeInMs" : 147850.0,
        "FinishedTasksDistributionInMs" : [ 87.0, 224.0, 265.0, 334.0, 8586.0 ]
      },
      "StartTimeInMs" : 1702226513989,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 5640,
        "IOBytes" : {
          "Total" : 1830628163,
          "Details" : {
            "IR" : 1830354688,
            "IW" : 0,
            "SR" : 162339,
            "SW" : 111136
          }
        }
      }
    },
    "6" : {
      "LQP" : {
        "operators" : {
          "8" : {
            "sign" : -988046795,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 17922960,
            "rowCount" : 248930,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          },
          "4" : {
            "sign" : -562937875,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 5696588771566252000,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#245 = d_week_seq#99) "
          },
          "9" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "5" : {
            "sign" : -1044119462,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))]) "
          },
          "6" : {
            "sign" : -514385727,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#245], BroadcastQueryStage 1 "
          },
          "1" : {
            "sign" : 1717045831,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 88224489309685684176756026,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "0" : {
            "sign" : 654300878,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 88224489309685684176756026,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "2" : {
            "sign" : -2096348315,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 91070440577740061085683640,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "7" : {
            "sign" : 566986461,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 16927240,
            "rowCount" : 248930,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "3" : {
            "sign" : 1796065461,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 5380111617590349111,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] "
          }
        },
        "links" : [ {
          "fromId" : 5,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Join",
          "toId" : 3,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "LogicalQueryStage",
          "toId" : 8,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "LogicalQueryStage",
          "toId" : 8,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Join",
          "toId" : 7,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n      :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n      :     :- LogicalQueryStage Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))])\n      :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- LogicalQueryStage Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))])\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "IM" : {
        "inputSizeInBytes" : 5430496686252,
        "inputRowCount" : 1412
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 16,
        "FinishedTasksNum" : 111,
        "FinishedTasksTotalTimeInMs" : 26765.0,
        "FinishedTasksDistributionInMs" : [ 144.0, 189.0, 215.0, 246.0, 534.0 ]
      },
      "StartTimeInMs" : 1702226516296,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 3333,
        "IOBytes" : {
          "Total" : 915399616,
          "Details" : {
            "IR" : 915177344,
            "IW" : 0,
            "SR" : 162339,
            "SW" : 59933
          }
        }
      }
    },
    "1" : {
      "LQP" : {
        "operators" : {
          "12" : {
            "sign" : 956450284,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] "
          },
          "8" : {
            "sign" : 963324291,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Union",
            "sizeInBytes" : 4305132820,
            "rowCount" : 215256641,
            "isRuntime" : false,
            "predicate" : " (unknown) Union Arguments: false, false "
          },
          "19" : {
            "sign" : -1945033734,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "sizeInBytes" : 680680,
            "rowCount" : 10010,
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "23" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "4" : {
            "sign" : 530772993,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 26280,
            "rowCount" : 365,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#245 = d_week_seq#99) "
          },
          "15" : {
            "sign" : 1586732036,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0 "
          },
          "11" : {
            "sign" : -966747353,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "sizeInBytes" : 14972593792,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [ws_sold_time_sk#27, ws_ship_date_sk#28, ws_item_sk#29, ws_bill_customer_sk#30, ws_bill_cdemo_sk#31, ws_bill_hdemo_sk#32, ws_bill_addr_sk#33, ws_ship_customer_sk#34, ws_ship_cdemo_sk#35, ws_ship_hdemo_sk#36, ws_ship_addr_sk#37, ws_web_page_sk#38, ws_web_site_sk#39, ws_ship_mode_sk#40, ws_warehouse_sk#41, ws_promo_sk#42, ws_order_number#43L, ws_quantity#44, ws_wholesale_cost#45, ws_list_price#46, ws_sales_price#47, ws_ext_discount_amt#48, ws_ext_sales_price#49, ws_ext_wholesale_cost#50, ... 10 more fields], `spark_catalog`.`tpcds_100`.`web_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "9" : {
            "sign" : 487343071,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] "
          },
          "22" : {
            "sign" : 1586732098,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2 "
          },
          "13" : {
            "sign" : -530295932,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "sizeInBytes" : 29800787536,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(cs_sold_date_sk#94) "
          },
          "16" : {
            "sign" : -514385727,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#245], BroadcastQueryStage 1 "
          },
          "5" : {
            "sign" : -1945033796,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "sizeInBytes" : 680680,
            "rowCount" : 10010,
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "10" : {
            "sign" : 224036230,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "sizeInBytes" : 14972593792,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(ws_sold_date_sk#60) "
          },
          "21" : {
            "sign" : 1964464157,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 754765553683632,
            "rowCount" : 15724282368409,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "6" : {
            "sign" : 1158039841,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 628971294736360,
            "rowCount" : 15724282368409,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "1" : {
            "sign" : 1945054853,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 16519900,
            "rowCount" : 133225,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "17" : {
            "sign" : -393680579,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 24820,
            "rowCount" : 365,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "14" : {
            "sign" : 1507442072,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "sizeInBytes" : 29800787536,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [cs_sold_time_sk#61, cs_ship_date_sk#62, cs_bill_customer_sk#63, cs_bill_cdemo_sk#64, cs_bill_hdemo_sk#65, cs_bill_addr_sk#66, cs_ship_customer_sk#67, cs_ship_cdemo_sk#68, cs_ship_hdemo_sk#69, cs_ship_addr_sk#70, cs_call_center_sk#71, cs_catalog_page_sk#72, cs_ship_mode_sk#73, cs_warehouse_sk#74, cs_item_sk#75, cs_promo_sk#76, cs_order_number#77L, cs_quantity#78, cs_wholesale_cost#79, cs_list_price#80, cs_sales_price#81, cs_ext_discount_amt#82, cs_ext_sales_price#83, cs_ext_wholesale_cost#84, ... 10 more fields], `spark_catalog`.`tpcds_100`.`catalog_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "0" : {
            "sign" : 971901188,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 16519900,
            "rowCount" : 133225,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "20" : {
            "sign" : 1158039903,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 628971294736360,
            "rowCount" : 15724282368409,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "2" : {
            "sign" : -230302309,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 17052800,
            "rowCount" : 133225,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "18" : {
            "sign" : 1341645003,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 26280,
            "rowCount" : 365,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          },
          "7" : {
            "sign" : 1964464095,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 754765553683632,
            "rowCount" : 15724282368409,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "3" : {
            "sign" : 145894295,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 24820,
            "rowCount" : 365,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] "
          }
        },
        "links" : [ {
          "fromId" : 11,
          "fromName" : "LogicalRelation",
          "toId" : 10,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 10,
          "fromName" : "Filter",
          "toId" : 9,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "Project",
          "toId" : 8,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 14,
          "fromName" : "LogicalRelation",
          "toId" : 13,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 13,
          "fromName" : "Filter",
          "toId" : 12,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 12,
          "fromName" : "Project",
          "toId" : 8,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Union",
          "toId" : 7,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 15,
          "fromName" : "LogicalQueryStage",
          "toId" : 7,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Join",
          "toId" : 6,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "Project",
          "toId" : 5,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "Aggregate",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 16,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Join",
          "toId" : 3,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Union",
          "toId" : 21,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 22,
          "fromName" : "LogicalQueryStage",
          "toId" : 21,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 21,
          "fromName" : "Join",
          "toId" : 20,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 20,
          "fromName" : "Project",
          "toId" : 19,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 19,
          "fromName" : "Aggregate",
          "toId" : 18,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 23,
          "fromName" : "LogicalQueryStage",
          "toId" : 18,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 18,
          "fromName" : "Join",
          "toId" : 17,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 17,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n      :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n      :     :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n      :     :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n      :     :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n      :     :        :- Union false, false\n      :     :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :     :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :     :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :     :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :     :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      :     :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n      :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n            :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n            :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n            :        :- Union false, false\n            :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n            :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n            :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n            :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n            :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n            :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n            :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "IM" : {
        "inputSizeInBytes" : 89554120184,
        "inputRowCount" : 430660110
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 2,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226501601,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 18028,
        "IOBytes" : {
          "Total" : 1831654067,
          "Details" : {
            "IR" : 1831374588,
            "IW" : 0,
            "SR" : 165341,
            "SW" : 114138
          }
        }
      }
    },
    "2" : {
      "LQP" : {
        "operators" : {
          "0" : {
            "sign" : -1224670454,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1888,
            "rowCount" : 1,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)]) "
          }
        },
        "links" : [ ],
        "rawPlan" : "LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)])\n"
      },
      "IM" : {
        "inputSizeInBytes" : 1888,
        "inputRowCount" : 1
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 1,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226501762,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 17867,
        "IOBytes" : {
          "Total" : 1831569727,
          "Details" : {
            "IR" : 1831291760,
            "IW" : 0,
            "SR" : 165341,
            "SW" : 112626
          }
        }
      }
    },
    "7" : {
      "LQP" : {
        "operators" : {
          "8" : {
            "sign" : -988046795,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 17922960,
            "rowCount" : 248930,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          },
          "4" : {
            "sign" : -562937875,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 17896680,
            "rowCount" : 248565,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#245 = d_week_seq#99) "
          },
          "9" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "5" : {
            "sign" : -1044119462,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 49032,
            "rowCount" : 681,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))]) "
          },
          "6" : {
            "sign" : -514385727,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#245], BroadcastQueryStage 1 "
          },
          "1" : {
            "sign" : 1717045831,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 7672535395800,
            "rowCount" : 61875285450,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "0" : {
            "sign" : 654300878,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "sizeInBytes" : 7672535395800,
            "rowCount" : 61875285450,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          },
          "2" : {
            "sign" : -2096348315,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "sizeInBytes" : 7920036537600,
            "rowCount" : 61875285450,
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "7" : {
            "sign" : 566986461,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 16927240,
            "rowCount" : 248930,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "3" : {
            "sign" : 1796065461,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "sizeInBytes" : 16902420,
            "rowCount" : 248565,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] "
          }
        },
        "links" : [ {
          "fromId" : 5,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "LogicalQueryStage",
          "toId" : 4,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Join",
          "toId" : 3,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "LogicalQueryStage",
          "toId" : 8,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "LogicalQueryStage",
          "toId" : 8,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Join",
          "toId" : 7,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Project",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n      :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n      :     :- LogicalQueryStage Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))])\n      :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- LogicalQueryStage Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240], HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))])\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "IM" : {
        "inputSizeInBytes" : 2196136,
        "inputRowCount" : 2093
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226518160,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 1469,
        "IOBytes" : {
          "Total" : 171107,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 162339,
            "SW" : 8768
          }
        }
      }
    },
    "3" : {
      "LQP" : {
        "operators" : {
          "0" : {
            "sign" : 700534141,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "sizeInBytes" : 1888,
            "rowCount" : 1,
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)]) "
          }
        },
        "links" : [ ],
        "rawPlan" : "LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)])\n"
      },
      "IM" : {
        "inputSizeInBytes" : 1888,
        "inputRowCount" : 1
      },
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 1,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "StartTimeInMs" : 1702226502053,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "Objectives" : {
        "DurationInMs" : 17576,
        "IOBytes" : {
          "Total" : 1831483897,
          "Details" : {
            "IR" : 1831208932,
            "IW" : 0,
            "SR" : 163829,
            "SW" : 111136
          }
        }
      }
    }
  },
  "RuntimeQSs" : {
    "12" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : 971901188,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Sort",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 123318823892972266,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 123318823892972266,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Arguments: [d_week_seq1#214 ASC NULLS FIRST], true "
          }
        },
        "links" : [ ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true\n+- Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n   +- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n      :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n      :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n      :     :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n      :     :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n      :     :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n      :     :        :- Union false, false\n      :     :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :     :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :     :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :     :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :     :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      :     :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n      :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n      +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n         +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n            :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n            :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n            :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n            :        :- Union false, false\n            :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n            :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n            :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n            :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n            :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n            :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n            :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2\n            +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -2093642880,
            "className" : "org.apache.spark.sql.execution.SortExec",
            "sizeInBytes" : 123318823892972266,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Sort Input [8]: [d_week_seq1#214, round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1 / sat_sales2), 2)#310] Arguments: [d_week_seq1#214 ASC NULLS FIRST], true, 0 "
          },
          "1" : {
            "sign" : 2458637,
            "className" : "org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec",
            "sizeInBytes" : 462392,
            "rowCount" : 2513,
            "isRuntime" : true,
            "predicate" : " (unknown) ShuffleQueryStage Output [8]: [d_week_seq1#214, round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1 / sat_sales2), 2)#310] Arguments: 7 "
          }
        },
        "links" : [ {
          "fromId" : 1,
          "fromName" : "ShuffleQueryStage",
          "toId" : 0,
          "toName" : "Sort",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Sort [d_week_seq1#214 ASC NULLS FIRST], true, 0\n+- ShuffleQueryStage 7\n   +- Exchange rangepartitioning(d_week_seq1#214 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=1050]\n      +- *(12) Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n         +- *(12) BroadcastHashJoin [d_week_seq1#214], [(d_week_seq2#222 - 53)], Inner, BuildLeft, false\n            :- BroadcastQueryStage 6\n            :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=989]\n            :     +- *(11) Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n            :        +- *(11) BroadcastHashJoin [d_week_seq#99], [d_week_seq#245], Inner, BuildRight, false\n            :           :- *(11) HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240])\n            :           :  +- AQEShuffleRead coalesced\n            :           :     +- ShuffleQueryStage 5\n            :           :        +- Exchange hashpartitioning(d_week_seq#99, 200), ENSURE_REQUIREMENTS, [plan_id=859]\n            :           :           +- *(10) HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L])\n            :           :              +- *(10) Project [sales_price#231, d_week_seq#99, d_day_name#109]\n            :           :                 +- *(10) BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n            :           :                    :- Union\n            :           :                    :  :- *(8) Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n            :           :                    :  :  +- *(8) ColumnarToRow\n            :           :                    :  :     +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n            :           :                    :  +- *(9) Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n            :           :                    :     +- *(9) ColumnarToRow\n            :           :                    :        +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n            :           :                    +- BroadcastQueryStage 0\n            :           :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=310]\n            :           :                          +- *(1) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#323, [id=#136], xxhash64(d_week_seq#99, 42)))\n            :           :                             :  +- Subquery subquery#323, [id=#136]\n            :           :                             :     +- AdaptiveSparkPlan isFinalPlan=true\n                                                               +- == Final Plan ==\n                                                                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                                                                  +- ShuffleQueryStage 0\n                                                                     +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=411]\n                                                                        +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                                                           +- *(1) Project [d_week_seq#245]\n                                                                              +- *(1) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                                                                 +- *(1) ColumnarToRow\n                                                                                    +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                                                               +- == Initial Plan ==\n                                                                  ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                                                                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]\n                                                                     +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                                                        +- Project [d_week_seq#245]\n                                                                           +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                                                              +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n            :           :                             +- *(1) ColumnarToRow\n            :           :                                +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n            :           +- BroadcastQueryStage 1\n            :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=332]\n            :                 +- *(2) Project [d_week_seq#245]\n            :                    +- *(2) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n            :                       +- *(2) ColumnarToRow\n            :                          +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n            +- *(12) Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n               +- *(12) BroadcastHashJoin [d_week_seq#99], [d_week_seq#273], Inner, BuildRight, false\n                  :- *(12) HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240])\n                  :  +- AQEShuffleRead coalesced\n                  :     +- ShuffleQueryStage 4\n                  :        +- Exchange hashpartitioning(d_week_seq#99, 200), ENSURE_REQUIREMENTS, [plan_id=728]\n                  :           +- *(7) HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#350L, sum#351L, sum#352L, sum#353L, sum#354L, sum#355L, sum#356L])\n                  :              +- *(7) Project [sales_price#231, d_week_seq#99, d_day_name#109]\n                  :                 +- *(7) BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n                  :                    :- Union\n                  :                    :  :- *(5) Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n                  :                    :  :  +- *(5) ColumnarToRow\n                  :                    :  :     +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n                  :                    :  +- *(6) Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n                  :                    :     +- *(6) ColumnarToRow\n                  :                    :        +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n                  :                    +- BroadcastQueryStage 2\n                  :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=351]\n                  :                          +- *(3) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#326, [id=#149], xxhash64(d_week_seq#99, 42)))\n                  :                             :  +- Subquery subquery#326, [id=#149]\n                  :                             :     +- AdaptiveSparkPlan isFinalPlan=true\n                                                         +- == Final Plan ==\n                                                            ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n                                                            +- ShuffleQueryStage 0\n                                                               +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=415]\n                                                                  +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n                                                                     +- *(1) Project [d_week_seq#273]\n                                                                        +- *(1) Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                                                                           +- *(1) ColumnarToRow\n                                                                              +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                                                         +- == Initial Plan ==\n                                                            ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n                                                            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=147]\n                                                               +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n                                                                  +- Project [d_week_seq#273]\n                                                                     +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                                                                        +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                  :                             +- *(3) ColumnarToRow\n                  :                                +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n                  +- BroadcastQueryStage 3\n                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=373]\n                        +- *(4) Project [d_week_seq#273]\n                           +- *(4) Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                              +- *(4) ColumnarToRow\n                                 +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 462392,
        "inputRowCount" : 2513
      },
      "InitialPartitionNum" : 200,
      "PD" : {
        "4" : [ 304, 171, 171, 171, 189, 171, 189, 189, 189, 171, 189, 171, 189, 171, 171, 189, 171, 171, 171, 189, 171, 171, 171, 171, 171, 171, 171, 171, 171, 189, 189, 171, 171, 171, 171, 171, 171, 171, 171, 171, 171, 171, 189, 171, 171, 171, 171, 171, 171, 171, 156, 156, 0 ]
      },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 12,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 22 ],
      "Objectives" : {
        "DurationInMs" : 202,
        "TotalTasksDurationInMs" : 192,
        "IOBytes" : {
          "Total" : 8768,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 8768,
            "SW" : 0
          }
        }
      }
    },
    "8" : {
      "QSLogical" : {
        "operators" : {
          "8" : {
            "sign" : -530295932,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              },
              "compileTime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(cs_sold_date_sk#94) "
          },
          "4" : {
            "sign" : 487343071,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1439672480,
                "rowCount" : 71983624
              },
              "compileTime" : {
                "sizeInBytes" : 1439672480,
                "rowCount" : 71983624
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] "
          },
          "9" : {
            "sign" : 1507442072,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              },
              "compileTime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [cs_sold_time_sk#61, cs_ship_date_sk#62, cs_bill_customer_sk#63, cs_bill_cdemo_sk#64, cs_bill_hdemo_sk#65, cs_bill_addr_sk#66, cs_ship_customer_sk#67, cs_ship_cdemo_sk#68, cs_ship_hdemo_sk#69, cs_ship_addr_sk#70, cs_call_center_sk#71, cs_catalog_page_sk#72, cs_ship_mode_sk#73, cs_warehouse_sk#74, cs_item_sk#75, cs_promo_sk#76, cs_order_number#77L, cs_quantity#78, cs_wholesale_cost#79, cs_list_price#80, cs_sales_price#81, cs_ext_discount_amt#82, cs_ext_sales_price#83, cs_ext_wholesale_cost#84, ... 10 more fields], `spark_catalog`.`tpcds_100`.`catalog_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "5" : {
            "sign" : 224036230,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              },
              "compileTime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(ws_sold_date_sk#60) "
          },
          "10" : {
            "sign" : 1586732098,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1051544,
                "rowCount" : 371
              },
              "compileTime" : {
                "sizeInBytes" : 2629764,
                "rowCount" : 73049
              }
            },
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2 "
          },
          "6" : {
            "sign" : -966747353,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              },
              "compileTime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [ws_sold_time_sk#27, ws_ship_date_sk#28, ws_item_sk#29, ws_bill_customer_sk#30, ws_bill_cdemo_sk#31, ws_bill_hdemo_sk#32, ws_bill_addr_sk#33, ws_ship_customer_sk#34, ws_ship_cdemo_sk#35, ws_ship_hdemo_sk#36, ws_ship_addr_sk#37, ws_web_page_sk#38, ws_web_site_sk#39, ws_ship_mode_sk#40, ws_warehouse_sk#41, ws_promo_sk#42, ws_order_number#43L, ws_quantity#44, ws_wholesale_cost#45, ws_list_price#46, ws_sales_price#47, ws_ext_discount_amt#48, ws_ext_sales_price#49, ws_ext_wholesale_cost#50, ... 10 more fields], `spark_catalog`.`tpcds_100`.`web_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "1" : {
            "sign" : 1158039903,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 3194408552440,
                "rowCount" : 79860213811
              },
              "compileTime" : {
                "sizeInBytes" : 3194408552440,
                "rowCount" : 79860213811
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "0" : {
            "sign" : -1945033734,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 5430494539148,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 5430494539148,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "2" : {
            "sign" : 1964464157,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 3833290262928,
                "rowCount" : 79860213811
              },
              "compileTime" : {
                "sizeInBytes" : 3833290262928,
                "rowCount" : 79860213811
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "7" : {
            "sign" : 956450284,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 2865460340,
                "rowCount" : 143273017
              },
              "compileTime" : {
                "sizeInBytes" : 2865460340,
                "rowCount" : 143273017
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] "
          },
          "3" : {
            "sign" : 963324291,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Union",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 4305132820,
                "rowCount" : 215256641
              },
              "compileTime" : {
                "sizeInBytes" : 4305132820,
                "rowCount" : 215256641
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Union Arguments: false, false "
          }
        },
        "links" : [ {
          "fromId" : 6,
          "fromName" : "LogicalRelation",
          "toId" : 5,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "Filter",
          "toId" : 4,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "LogicalRelation",
          "toId" : 8,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Filter",
          "toId" : 7,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Union",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 10,
          "fromName" : "LogicalQueryStage",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n+- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n      :- Union false, false\n      :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2\n"
      },
      "QSPhysical" : {
        "operators" : {
          "8" : {
            "sign" : -1983665744,
            "className" : "org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec",
            "sizeInBytes" : 1051544,
            "rowCount" : 371,
            "isRuntime" : true,
            "predicate" : " (unknown) BroadcastQueryStage Output [3]: [d_date_sk#95, d_week_seq#99, d_day_name#109] Arguments: 2 "
          },
          "4" : {
            "sign" : 1120857500,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [2]: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] Input [2]: [ws_ext_sales_price#49, ws_sold_date_sk#60] "
          },
          "5" : {
            "sign" : -841138551,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.web_sales Output [2]: [ws_ext_sales_price#49, ws_sold_date_sk#60] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sales/ws_sold_date_sk=2450816, ... 1822 entries] PartitionFilters: [isnotnull(ws_sold_date_sk#60)] ReadSchema: struct<ws_ext_sales_price:decimal(7,2)> "
          },
          "6" : {
            "sign" : 454650011,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [2]: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] Input [2]: [cs_ext_sales_price#83, cs_sold_date_sk#94] "
          },
          "1" : {
            "sign" : 1952771513,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 3194408552440,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [3]: [sales_price#231, d_week_seq#99, d_day_name#109] Input [5]: [sold_date_sk#230, sales_price#231, d_date_sk#95, d_week_seq#99, d_day_name#109] "
          },
          "0" : {
            "sign" : -1751769436,
            "className" : "org.apache.spark.sql.execution.aggregate.HashAggregateExec",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) HashAggregate Input [3]: [sales_price#231, d_week_seq#99, d_day_name#109] Keys [1]: [d_week_seq#99] Functions [7]: [partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))] Aggregate Attributes [7]: [sum#343L, sum#344L, sum#345L, sum#346L, sum#347L, sum#348L, sum#349L] Results [8]: [d_week_seq#99, sum#350L, sum#351L, sum#352L, sum#353L, sum#354L, sum#355L, sum#356L] "
          },
          "2" : {
            "sign" : 503357433,
            "className" : "org.apache.spark.sql.execution.joins.BroadcastHashJoinExec",
            "sizeInBytes" : 3833290262928,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) BroadcastHashJoin Left keys [1]: [sold_date_sk#230] Right keys [1]: [d_date_sk#95] Join type: Inner Join condition: None "
          },
          "7" : {
            "sign" : 327011135,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.catalog_sales Output [2]: [cs_ext_sales_price#83, cs_sold_date_sk#94] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalog_sales/cs_sold_date_sk=2450815, ... 1835 entries] PartitionFilters: [isnotnull(cs_sold_date_sk#94)] ReadSchema: struct<cs_ext_sales_price:decimal(7,2)> "
          },
          "3" : {
            "sign" : -1864301529,
            "className" : "org.apache.spark.sql.execution.UnionExec",
            "sizeInBytes" : 4305132820,
            "rowCount" : 215256641,
            "isRuntime" : false,
            "predicate" : " (unknown) Union "
          }
        },
        "links" : [ {
          "fromId" : 5,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.web_sales",
          "toId" : 4,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.catalog_sales",
          "toId" : 6,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Union",
          "toId" : 2,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "BroadcastQueryStage",
          "toId" : 2,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "BroadcastHashJoin",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "HashAggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#350L, sum#351L, sum#352L, sum#353L, sum#354L, sum#355L, sum#356L])\n+- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   +- BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n      :- Union\n      :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :  :  +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n      :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n      +- BroadcastQueryStage 2\n         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=351]\n            +- *(3) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#326, [id=#149], xxhash64(d_week_seq#99, 42)))\n               :  +- Subquery subquery#326, [id=#149]\n               :     +- AdaptiveSparkPlan isFinalPlan=true\n                        +- == Final Plan ==\n                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n                           +- ShuffleQueryStage 0\n                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=415]\n                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n                                    +- *(1) Project [d_week_seq#273]\n                                       +- *(1) Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                                          +- *(1) ColumnarToRow\n                                             +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                        +- == Initial Plan ==\n                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=147]\n                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n                                 +- Project [d_week_seq#273]\n                                    +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                                       +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n               +- *(3) ColumnarToRow\n                  +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 4306184364,
        "inputRowCount" : 215257012
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 1,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 8,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 11 ],
      "Objectives" : {
        "DurationInMs" : 12571,
        "TotalTasksDurationInMs" : 170181,
        "IOBytes" : {
          "Total" : 915228547,
          "Details" : {
            "IR" : 915177344,
            "IW" : 0,
            "SR" : 0,
            "SW" : 51203
          }
        }
      }
    },
    "4" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : -1167954948,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 108,
                "rowCount" : 1
              },
              "compileTime" : {
                "sizeInBytes" : 108,
                "rowCount" : 1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325] "
          }
        },
        "links" : [ ],
        "rawPlan" : "Aggregate [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325]\n+- Project [d_week_seq#273]\n   +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n      +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#269,d_date_id#270,d_date#271,d_month_seq#272,d_week_seq#273,d_quarter_seq#274,d_year#275,d_dow#276,d_moy#277,d_dom#278,d_qoy#279,d_fy_year#280,d_fy_quarter_seq#281,d_fy_week_seq#282,d_day_name#283,d_quarter_name#284,d_holiday#285,d_weekend#286,d_following_holiday#287,d_first_dom#288,d_last_dom#289,d_same_day_ly#290,d_same_day_lq#291,d_current_day#292,... 4 more fields] parquet\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : 302941996,
            "className" : "org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec",
            "sizeInBytes" : 108,
            "rowCount" : 1,
            "isRuntime" : false,
            "predicate" : " (unknown) ObjectHashAggregate Input [1]: [d_week_seq#273] Keys: [] Functions [1]: [partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)] Aggregate Attributes [1]: [buf#327] Results [1]: [buf#358] "
          },
          "1" : {
            "sign" : 1278028323,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [1]: [d_week_seq#273] Input [2]: [d_week_seq#273, d_year#275] "
          },
          "2" : {
            "sign" : 1227927444,
            "className" : "org.apache.spark.sql.execution.FilterExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Input [2]: [d_week_seq#273, d_year#275] Condition : ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273)) "
          },
          "3" : {
            "sign" : -1111050904,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.date_dim Output [2]: [d_week_seq#273, d_year#275] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim] PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)] ReadSchema: struct<d_week_seq:int,d_year:int> "
          }
        },
        "links" : [ {
          "fromId" : 3,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.date_dim",
          "toId" : 2,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Filter",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "ObjectHashAggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n+- Project [d_week_seq#273]\n   +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n      +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 6912,
        "inputRowCount" : 576
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 5,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 5 ],
      "Objectives" : {
        "DurationInMs" : 1708,
        "TotalTasksDurationInMs" : 1703,
        "IOBytes" : {
          "Total" : 82828,
          "Details" : {
            "IR" : 82828,
            "IW" : 0,
            "SR" : 0,
            "SW" : 0
          }
        }
      }
    },
    "11" : {
      "QSLogical" : {
        "operators" : {
          "4" : {
            "sign" : -1986550524,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1049000,
                "rowCount" : 365
              },
              "compileTime" : {
                "sizeInBytes" : 6912,
                "rowCount" : 576
              }
            },
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#273], BroadcastQueryStage 3 "
          },
          "1" : {
            "sign" : -230302309,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 127296850470164920,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 127296850470164920,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53)) "
          },
          "0" : {
            "sign" : 1945054853,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 123318823892972266,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 123318823892972266,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] "
          },
          "2" : {
            "sign" : -393680579,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 5128801388806,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 5128801388806,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] "
          },
          "3" : {
            "sign" : 1341645003,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 5430495588148,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 5430495588148,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#273 = d_week_seq#99) "
          }
        },
        "links" : [ {
          "fromId" : 4,
          "fromName" : "LogicalQueryStage",
          "toId" : 3,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Join",
          "toId" : 2,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Project",
          "toId" : 1,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Join",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n+- Join Inner, (d_week_seq1#214 = (d_week_seq2#222 - 53))\n   :- Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n   :  +- Join Inner, (d_week_seq#245 = d_week_seq#99)\n   :     :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n   :     :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   :     :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n   :     :        :- Union false, false\n   :     :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n   :     :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n   :     :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n   :     :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n   :     :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n   :     :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n   :     :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n   :     +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n   +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n      +- Join Inner, (d_week_seq#273 = d_week_seq#99)\n         :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n         :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n         :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n         :        :- Union false, false\n         :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n         :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n         :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n         :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n         :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n         :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n         :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 2\n         +- LogicalQueryStage Project [d_week_seq#273], BroadcastQueryStage 3\n"
      },
      "QSPhysical" : {
        "operators" : {
          "4" : {
            "sign" : 265368356,
            "className" : "org.apache.spark.sql.execution.joins.BroadcastHashJoinExec",
            "sizeInBytes" : 5430495588148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) BroadcastHashJoin Left keys [1]: [d_week_seq#99] Right keys [1]: [d_week_seq#273] Join type: Inner Join condition: None "
          },
          "5" : {
            "sign" : 130560386,
            "className" : "org.apache.spark.sql.execution.aggregate.HashAggregateExec",
            "sizeInBytes" : 49104,
            "rowCount" : 682,
            "isRuntime" : true,
            "predicate" : " (unknown) HashAggregate Input [8]: [d_week_seq#99, sum#350L, sum#351L, sum#352L, sum#353L, sum#354L, sum#355L, sum#356L] Keys [1]: [d_week_seq#99] Functions [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))] Aggregate Attributes [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END))#297L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END))#298L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END))#299L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END))#300L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END))#301L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END))#302L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))#303L] Results [8]: [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END))#297L,17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END))#298L,17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END))#299L,17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END))#300L,17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END))#301L,17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END))#302L,17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))#303L,17,2) AS sat_sales#240] "
          },
          "6" : {
            "sign" : 2029739087,
            "className" : "org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) ShuffleQueryStage Output [8]: [d_week_seq#99, sum#350L, sum#351L, sum#352L, sum#353L, sum#354L, sum#355L, sum#356L] Arguments: 4 "
          },
          "1" : {
            "sign" : 1805668523,
            "className" : "org.apache.spark.sql.execution.joins.BroadcastHashJoinExec",
            "sizeInBytes" : 127296850470164920,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) BroadcastHashJoin Left keys [1]: [d_week_seq1#214] Right keys [1]: [(d_week_seq2#222 - 53)] Join type: Inner Join condition: None "
          },
          "0" : {
            "sign" : 2101431045,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 123318823892972266,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [8]: [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310] Input [16]: [d_week_seq1#214, sun_sales1#215, mon_sales1#216, tue_sales1#217, wed_sales1#218, thu_sales1#219, fri_sales1#220, sat_sales1#221, d_week_seq2#222, sun_sales2#223, mon_sales2#224, tue_sales2#225, wed_sales2#226, thu_sales2#227, fri_sales2#228, sat_sales2#229] "
          },
          "2" : {
            "sign" : 1509512489,
            "className" : "org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) BroadcastQueryStage Output [8]: [d_week_seq1#214, sun_sales1#215, mon_sales1#216, tue_sales1#217, wed_sales1#218, thu_sales1#219, fri_sales1#220, sat_sales1#221] Arguments: 6 "
          },
          "7" : {
            "sign" : -1738388063,
            "className" : "org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) BroadcastQueryStage Output [1]: [d_week_seq#273] Arguments: 3 "
          },
          "3" : {
            "sign" : 304030812,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 5128801388806,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [8]: [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229] Input [9]: [d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240, d_week_seq#273] "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "BroadcastQueryStage",
          "toId" : 1,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "ShuffleQueryStage",
          "toId" : 5,
          "toName" : "HashAggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "HashAggregate",
          "toId" : 4,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "BroadcastQueryStage",
          "toId" : 4,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "BroadcastHashJoin",
          "toId" : 3,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Project",
          "toId" : 1,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "BroadcastHashJoin",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq1#214, round((sun_sales1#215 / sun_sales2#223), 2) AS round((sun_sales1 / sun_sales2), 2)#304, round((mon_sales1#216 / mon_sales2#224), 2) AS round((mon_sales1 / mon_sales2), 2)#305, round((tue_sales1#217 / tue_sales2#225), 2) AS round((tue_sales1 / tue_sales2), 2)#306, round((wed_sales1#218 / wed_sales2#226), 2) AS round((wed_sales1 / wed_sales2), 2)#307, round((thu_sales1#219 / thu_sales2#227), 2) AS round((thu_sales1 / thu_sales2), 2)#308, round((fri_sales1#220 / fri_sales2#228), 2) AS round((fri_sales1 / fri_sales2), 2)#309, round((sat_sales1#221 / sat_sales2#229), 2) AS round((sat_sales1 / sat_sales2), 2)#310]\n+- BroadcastHashJoin [d_week_seq1#214], [(d_week_seq2#222 - 53)], Inner, BuildLeft, false\n   :- BroadcastQueryStage 6\n   :  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=989]\n   :     +- *(11) Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n   :        +- *(11) BroadcastHashJoin [d_week_seq#99], [d_week_seq#245], Inner, BuildRight, false\n   :           :- *(11) HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240])\n   :           :  +- AQEShuffleRead coalesced\n   :           :     +- ShuffleQueryStage 5\n   :           :        +- Exchange hashpartitioning(d_week_seq#99, 200), ENSURE_REQUIREMENTS, [plan_id=859]\n   :           :           +- *(10) HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L])\n   :           :              +- *(10) Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   :           :                 +- *(10) BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n   :           :                    :- Union\n   :           :                    :  :- *(8) Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n   :           :                    :  :  +- *(8) ColumnarToRow\n   :           :                    :  :     +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n   :           :                    :  +- *(9) Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n   :           :                    :     +- *(9) ColumnarToRow\n   :           :                    :        +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n   :           :                    +- BroadcastQueryStage 0\n   :           :                       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=310]\n   :           :                          +- *(1) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#323, [id=#136], xxhash64(d_week_seq#99, 42)))\n   :           :                             :  +- Subquery subquery#323, [id=#136]\n   :           :                             :     +- AdaptiveSparkPlan isFinalPlan=true\n                                                      +- == Final Plan ==\n                                                         ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                                                         +- ShuffleQueryStage 0\n                                                            +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=411]\n                                                               +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                                                  +- *(1) Project [d_week_seq#245]\n                                                                     +- *(1) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                                                        +- *(1) ColumnarToRow\n                                                                           +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                                                      +- == Initial Plan ==\n                                                         ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                                                         +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]\n                                                            +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                                               +- Project [d_week_seq#245]\n                                                                  +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                                                     +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n   :           :                             +- *(1) ColumnarToRow\n   :           :                                +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n   :           +- BroadcastQueryStage 1\n   :              +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=332]\n   :                 +- *(2) Project [d_week_seq#245]\n   :                    +- *(2) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n   :                       +- *(2) ColumnarToRow\n   :                          +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n   +- Project [d_week_seq#99 AS d_week_seq2#222, sun_sales#234 AS sun_sales2#223, mon_sales#235 AS mon_sales2#224, tue_sales#236 AS tue_sales2#225, wed_sales#237 AS wed_sales2#226, thu_sales#238 AS thu_sales2#227, fri_sales#239 AS fri_sales2#228, sat_sales#240 AS sat_sales2#229]\n      +- BroadcastHashJoin [d_week_seq#99], [d_week_seq#273], Inner, BuildRight, false\n         :- HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240])\n         :  +- ShuffleQueryStage 4\n         :     +- Exchange hashpartitioning(d_week_seq#99, 200), ENSURE_REQUIREMENTS, [plan_id=728]\n         :        +- *(7) HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#350L, sum#351L, sum#352L, sum#353L, sum#354L, sum#355L, sum#356L])\n         :           +- *(7) Project [sales_price#231, d_week_seq#99, d_day_name#109]\n         :              +- *(7) BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n         :                 :- Union\n         :                 :  :- *(5) Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n         :                 :  :  +- *(5) ColumnarToRow\n         :                 :  :     +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n         :                 :  +- *(6) Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n         :                 :     +- *(6) ColumnarToRow\n         :                 :        +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n         :                 +- BroadcastQueryStage 2\n         :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=351]\n         :                       +- *(3) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#326, [id=#149], xxhash64(d_week_seq#99, 42)))\n         :                          :  +- Subquery subquery#326, [id=#149]\n         :                          :     +- AdaptiveSparkPlan isFinalPlan=true\n                                             +- == Final Plan ==\n                                                ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n                                                +- ShuffleQueryStage 0\n                                                   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=415]\n                                                      +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n                                                         +- *(1) Project [d_week_seq#273]\n                                                            +- *(1) Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                                                               +- *(1) ColumnarToRow\n                                                                  +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                                             +- == Initial Plan ==\n                                                ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n                                                +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=147]\n                                                   +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n                                                      +- Project [d_week_seq#273]\n                                                         +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                                                            +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n         :                          +- *(3) ColumnarToRow\n         :                             +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n         +- BroadcastQueryStage 3\n            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=373]\n               +- *(4) Project [d_week_seq#273]\n                  +- *(4) Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n                     +- *(4) ColumnarToRow\n                        +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 2147104,
        "inputRowCount" : 1412
      },
      "InitialPartitionNum" : 200,
      "PD" : {
        "2" : [ 0, 0, 881, 0, 0, 0, 0, 0, 0, 720, 0, 1104, 0, 0, 0, 0, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 1104, 1032, 1848, 0, 0, 0, 0, 0, 0, 1032, 0, 1920, 0, 2064, 1992, 0, 0, 0, 0, 0, 1946, 0, 0, 0, 1104, 0, 960, 2064, 0, 960, 1104, 969, 0, 1104, 0, 960, 0, 0, 0, 0, 0, 0, 0, 1104, 0, 0, 1032, 0, 0, 0, 0, 0, 1104, 0, 0, 1104, 0, 0, 0, 0, 0, 0, 888, 0, 1032, 1032, 0, 1104, 0, 0, 0, 952, 0, 0, 0, 0, 0, 1032, 0, 1032, 0, 0, 0, 0, 0, 0, 0, 0, 960, 0, 0, 0, 952, 0, 0, 0, 0, 0, 0, 0, 0, 1104, 0, 0, 0, 1032, 0, 0, 1104, 1032, 0, 0, 0, 0, 0, 0, 888, 0, 0, 0, 0, 0, 1104, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1104, 0, 1032, 1032, 0, 0, 0, 0, 0, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1992, 0, 0, 0, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]
      },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 11,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 17, 19 ],
      "Objectives" : {
        "DurationInMs" : 564,
        "TotalTasksDurationInMs" : 528,
        "IOBytes" : {
          "Total" : 111174,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 102406,
            "SW" : 8768
          }
        }
      }
    },
    "9" : {
      "QSLogical" : {
        "operators" : {
          "8" : {
            "sign" : -530295932,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              },
              "compileTime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(cs_sold_date_sk#94) "
          },
          "4" : {
            "sign" : 487343071,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1439672480,
                "rowCount" : 71983624
              },
              "compileTime" : {
                "sizeInBytes" : 1439672480,
                "rowCount" : 71983624
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] "
          },
          "9" : {
            "sign" : 1507442072,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              },
              "compileTime" : {
                "sizeInBytes" : 29800787536,
                "rowCount" : 143273017
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [cs_sold_time_sk#61, cs_ship_date_sk#62, cs_bill_customer_sk#63, cs_bill_cdemo_sk#64, cs_bill_hdemo_sk#65, cs_bill_addr_sk#66, cs_ship_customer_sk#67, cs_ship_cdemo_sk#68, cs_ship_hdemo_sk#69, cs_ship_addr_sk#70, cs_call_center_sk#71, cs_catalog_page_sk#72, cs_ship_mode_sk#73, cs_warehouse_sk#74, cs_item_sk#75, cs_promo_sk#76, cs_order_number#77L, cs_quantity#78, cs_wholesale_cost#79, cs_list_price#80, cs_sales_price#81, cs_ext_discount_amt#82, cs_ext_sales_price#83, cs_ext_wholesale_cost#84, ... 10 more fields], `spark_catalog`.`tpcds_100`.`catalog_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "5" : {
            "sign" : 224036230,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              },
              "compileTime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: isnotnull(ws_sold_date_sk#60) "
          },
          "10" : {
            "sign" : 1586732036,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1051544,
                "rowCount" : 371
              },
              "compileTime" : {
                "sizeInBytes" : 2629764,
                "rowCount" : 73049
              }
            },
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0 "
          },
          "6" : {
            "sign" : -966747353,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              },
              "compileTime" : {
                "sizeInBytes" : 14972593792,
                "rowCount" : 71983624
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [ws_sold_time_sk#27, ws_ship_date_sk#28, ws_item_sk#29, ws_bill_customer_sk#30, ws_bill_cdemo_sk#31, ws_bill_hdemo_sk#32, ws_bill_addr_sk#33, ws_ship_customer_sk#34, ws_ship_cdemo_sk#35, ws_ship_hdemo_sk#36, ws_ship_addr_sk#37, ws_web_page_sk#38, ws_web_site_sk#39, ws_ship_mode_sk#40, ws_warehouse_sk#41, ws_promo_sk#42, ws_order_number#43L, ws_quantity#44, ws_wholesale_cost#45, ws_list_price#46, ws_sales_price#47, ws_ext_discount_amt#48, ws_ext_sales_price#49, ws_ext_wholesale_cost#50, ... 10 more fields], `spark_catalog`.`tpcds_100`.`web_sales`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          },
          "1" : {
            "sign" : 1158039841,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 3194408552440,
                "rowCount" : 79860213811
              },
              "compileTime" : {
                "sizeInBytes" : 3194408552440,
                "rowCount" : 79860213811
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [sales_price#231, d_week_seq#99, d_day_name#109] "
          },
          "0" : {
            "sign" : -1945033796,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 5430494539148,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 5430494539148,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240] "
          },
          "2" : {
            "sign" : 1964464095,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 3833290262928,
                "rowCount" : 79860213811
              },
              "compileTime" : {
                "sizeInBytes" : 3833290262928,
                "rowCount" : 79860213811
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_date_sk#95 = sold_date_sk#230) "
          },
          "7" : {
            "sign" : 956450284,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 2865460340,
                "rowCount" : 143273017
              },
              "compileTime" : {
                "sizeInBytes" : 2865460340,
                "rowCount" : 143273017
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] "
          },
          "3" : {
            "sign" : 963324291,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Union",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 4305132820,
                "rowCount" : 215256641
              },
              "compileTime" : {
                "sizeInBytes" : 4305132820,
                "rowCount" : 215256641
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Union Arguments: false, false "
          }
        },
        "links" : [ {
          "fromId" : 6,
          "fromName" : "LogicalRelation",
          "toId" : 5,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 5,
          "fromName" : "Filter",
          "toId" : 4,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 9,
          "fromName" : "LogicalRelation",
          "toId" : 8,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "Filter",
          "toId" : 7,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Union",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 10,
          "fromName" : "LogicalQueryStage",
          "toId" : 2,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Join",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "Aggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n+- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n      :- Union false, false\n      :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n      :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n      :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     +- Filter isnotnull(cs_sold_date_sk#94)\n      :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n      +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n"
      },
      "QSPhysical" : {
        "operators" : {
          "8" : {
            "sign" : 2120789931,
            "className" : "org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec",
            "sizeInBytes" : 1051544,
            "rowCount" : 371,
            "isRuntime" : true,
            "predicate" : " (unknown) BroadcastQueryStage Output [3]: [d_date_sk#95, d_week_seq#99, d_day_name#109] Arguments: 0 "
          },
          "4" : {
            "sign" : 1120857500,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [2]: [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231] Input [2]: [ws_ext_sales_price#49, ws_sold_date_sk#60] "
          },
          "5" : {
            "sign" : -841138551,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 1439672480,
            "rowCount" : 71983624,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.web_sales Output [2]: [ws_ext_sales_price#49, ws_sold_date_sk#60] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sales/ws_sold_date_sk=2450816, ... 1822 entries] PartitionFilters: [isnotnull(ws_sold_date_sk#60)] ReadSchema: struct<ws_ext_sales_price:decimal(7,2)> "
          },
          "6" : {
            "sign" : 454650011,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [2]: [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233] Input [2]: [cs_ext_sales_price#83, cs_sold_date_sk#94] "
          },
          "1" : {
            "sign" : 2046262900,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 3194408552440,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [3]: [sales_price#231, d_week_seq#99, d_day_name#109] Input [5]: [sold_date_sk#230, sales_price#231, d_date_sk#95, d_week_seq#99, d_day_name#109] "
          },
          "0" : {
            "sign" : -1594285326,
            "className" : "org.apache.spark.sql.execution.aggregate.HashAggregateExec",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) HashAggregate Input [3]: [sales_price#231, d_week_seq#99, d_day_name#109] Keys [1]: [d_week_seq#99] Functions [7]: [partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))] Aggregate Attributes [7]: [sum#329L, sum#330L, sum#331L, sum#332L, sum#333L, sum#334L, sum#335L] Results [8]: [d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L] "
          },
          "2" : {
            "sign" : 1349369054,
            "className" : "org.apache.spark.sql.execution.joins.BroadcastHashJoinExec",
            "sizeInBytes" : 3833290262928,
            "rowCount" : 79860213811,
            "isRuntime" : false,
            "predicate" : " (unknown) BroadcastHashJoin Left keys [1]: [sold_date_sk#230] Right keys [1]: [d_date_sk#95] Join type: Inner Join condition: None "
          },
          "7" : {
            "sign" : 327011135,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 2865460340,
            "rowCount" : 143273017,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.catalog_sales Output [2]: [cs_ext_sales_price#83, cs_sold_date_sk#94] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalog_sales/cs_sold_date_sk=2450815, ... 1835 entries] PartitionFilters: [isnotnull(cs_sold_date_sk#94)] ReadSchema: struct<cs_ext_sales_price:decimal(7,2)> "
          },
          "3" : {
            "sign" : -1864301529,
            "className" : "org.apache.spark.sql.execution.UnionExec",
            "sizeInBytes" : 4305132820,
            "rowCount" : 215256641,
            "isRuntime" : false,
            "predicate" : " (unknown) Union "
          }
        },
        "links" : [ {
          "fromId" : 5,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.web_sales",
          "toId" : 4,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 7,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.catalog_sales",
          "toId" : 6,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 6,
          "fromName" : "Project",
          "toId" : 3,
          "toName" : "Union",
          "linkType" : "Operator"
        }, {
          "fromId" : 3,
          "fromName" : "Union",
          "toId" : 2,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 8,
          "fromName" : "BroadcastQueryStage",
          "toId" : 2,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "BroadcastHashJoin",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "HashAggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L])\n+- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   +- BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n      :- Union\n      :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n      :  :  +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n      :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n      :     +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n      +- BroadcastQueryStage 0\n         +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=310]\n            +- *(1) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#323, [id=#136], xxhash64(d_week_seq#99, 42)))\n               :  +- Subquery subquery#323, [id=#136]\n               :     +- AdaptiveSparkPlan isFinalPlan=true\n                        +- == Final Plan ==\n                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                           +- ShuffleQueryStage 0\n                              +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=411]\n                                 +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                    +- *(1) Project [d_week_seq#245]\n                                       +- *(1) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                          +- *(1) ColumnarToRow\n                                             +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                        +- == Initial Plan ==\n                           ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]\n                              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                 +- Project [d_week_seq#245]\n                                    +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                       +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n               +- *(1) ColumnarToRow\n                  +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 4306184364,
        "inputRowCount" : 215257012
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 6,
        "FinishedTasksNum" : 229,
        "FinishedTasksTotalTimeInMs" : 148997.0,
        "FinishedTasksDistributionInMs" : [ 87.0, 223.0, 264.0, 333.0, 8586.0 ]
      },
      "QueryStageOptimizationId" : 9,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 13 ],
      "Objectives" : {
        "DurationInMs" : 3928,
        "TotalTasksDurationInMs" : 53891,
        "IOBytes" : {
          "Total" : 915228509,
          "Details" : {
            "IR" : 915177344,
            "IW" : 0,
            "SR" : 0,
            "SW" : 51165
          }
        }
      }
    },
    "5" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : -1224670454,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1888,
                "rowCount" : 1
              },
              "compileTime" : {
                "sizeInBytes" : 108,
                "rowCount" : 1
              }
            },
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)]) "
          }
        },
        "links" : [ ],
        "rawPlan" : "LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)])\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -168145842,
            "className" : "org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec",
            "sizeInBytes" : 1888,
            "rowCount" : 1,
            "isRuntime" : true,
            "predicate" : " (unknown) ObjectHashAggregate Input [1]: [buf#358] Keys: [] Functions [1]: [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)] Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)#324] Results [1]: [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)#324 AS bloomFilter#325] "
          },
          "1" : {
            "sign" : -925660841,
            "className" : "org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec",
            "sizeInBytes" : 108,
            "rowCount" : 1,
            "isRuntime" : false,
            "predicate" : " (unknown) ShuffleQueryStage Output [1]: [buf#358] Arguments: 0 "
          }
        },
        "links" : [ {
          "fromId" : 1,
          "fromName" : "ShuffleQueryStage",
          "toId" : 0,
          "toName" : "ObjectHashAggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n+- ShuffleQueryStage 0\n   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=415]\n      +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n         +- *(1) Project [d_week_seq#273]\n            +- *(1) Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n               +- *(1) ColumnarToRow\n                  +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 1888,
        "inputRowCount" : 1
      },
      "InitialPartitionNum" : 1,
      "PD" : {
        "0" : [ 1538 ]
      },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 1,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 6,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 10 ],
      "Objectives" : {
        "DurationInMs" : 1772,
        "TotalTasksDurationInMs" : 1757,
        "IOBytes" : {
          "Total" : 1490,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 1490,
            "SW" : 0
          }
        }
      }
    },
    "10" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : 145894295,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 5128801388806,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 5128801388806,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] "
          },
          "1" : {
            "sign" : 530772993,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Join",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 5430495588148,
                "rowCount" : -1
              },
              "compileTime" : {
                "sizeInBytes" : 5430495588148,
                "rowCount" : -1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Join Arguments: Inner, (d_week_seq#245 = d_week_seq#99) "
          },
          "2" : {
            "sign" : -514385727,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1049000,
                "rowCount" : 365
              },
              "compileTime" : {
                "sizeInBytes" : 6912,
                "rowCount" : 576
              }
            },
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Project [d_week_seq#245], BroadcastQueryStage 1 "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "LogicalQueryStage",
          "toId" : 1,
          "toName" : "Join",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Join",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n+- Join Inner, (d_week_seq#245 = d_week_seq#99)\n   :- Aggregate [d_week_seq#99], [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)),17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)),17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)),17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)),17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)),17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)),17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END)),17,2) AS sat_sales#240]\n   :  +- Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   :     +- Join Inner, (d_date_sk#95 = sold_date_sk#230)\n   :        :- Union false, false\n   :        :  :- Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n   :        :  :  +- Filter isnotnull(ws_sold_date_sk#60)\n   :        :  :     +- Relation spark_catalog.tpcds_100.web_sales[ws_sold_time_sk#27,ws_ship_date_sk#28,ws_item_sk#29,ws_bill_customer_sk#30,ws_bill_cdemo_sk#31,ws_bill_hdemo_sk#32,ws_bill_addr_sk#33,ws_ship_customer_sk#34,ws_ship_cdemo_sk#35,ws_ship_hdemo_sk#36,ws_ship_addr_sk#37,ws_web_page_sk#38,ws_web_site_sk#39,ws_ship_mode_sk#40,ws_warehouse_sk#41,ws_promo_sk#42,ws_order_number#43L,ws_quantity#44,ws_wholesale_cost#45,ws_list_price#46,ws_sales_price#47,ws_ext_discount_amt#48,ws_ext_sales_price#49,ws_ext_wholesale_cost#50,... 10 more fields] parquet\n   :        :  +- Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n   :        :     +- Filter isnotnull(cs_sold_date_sk#94)\n   :        :        +- Relation spark_catalog.tpcds_100.catalog_sales[cs_sold_time_sk#61,cs_ship_date_sk#62,cs_bill_customer_sk#63,cs_bill_cdemo_sk#64,cs_bill_hdemo_sk#65,cs_bill_addr_sk#66,cs_ship_customer_sk#67,cs_ship_cdemo_sk#68,cs_ship_hdemo_sk#69,cs_ship_addr_sk#70,cs_call_center_sk#71,cs_catalog_page_sk#72,cs_ship_mode_sk#73,cs_warehouse_sk#74,cs_item_sk#75,cs_promo_sk#76,cs_order_number#77L,cs_quantity#78,cs_wholesale_cost#79,cs_list_price#80,cs_sales_price#81,cs_ext_discount_amt#82,cs_ext_sales_price#83,cs_ext_wholesale_cost#84,... 10 more fields] parquet\n   :        +- LogicalQueryStage Project [d_date_sk#95, d_week_seq#99, d_day_name#109], BroadcastQueryStage 0\n   +- LogicalQueryStage Project [d_week_seq#245], BroadcastQueryStage 1\n"
      },
      "QSPhysical" : {
        "operators" : {
          "4" : {
            "sign" : -878937764,
            "className" : "org.apache.spark.sql.execution.adaptive.BroadcastQueryStageExec",
            "sizeInBytes" : 1049000,
            "rowCount" : 365,
            "isRuntime" : true,
            "predicate" : " (unknown) BroadcastQueryStage Output [1]: [d_week_seq#245] Arguments: 1 "
          },
          "1" : {
            "sign" : 1258319880,
            "className" : "org.apache.spark.sql.execution.joins.BroadcastHashJoinExec",
            "sizeInBytes" : 5430495588148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) BroadcastHashJoin Left keys [1]: [d_week_seq#99] Right keys [1]: [d_week_seq#245] Join type: Inner Join condition: None "
          },
          "0" : {
            "sign" : -1641697250,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 5128801388806,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [8]: [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221] Input [9]: [d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240, d_week_seq#245] "
          },
          "2" : {
            "sign" : 915263820,
            "className" : "org.apache.spark.sql.execution.aggregate.HashAggregateExec",
            "sizeInBytes" : 49032,
            "rowCount" : 681,
            "isRuntime" : true,
            "predicate" : " (unknown) HashAggregate Input [8]: [d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L] Keys [1]: [d_week_seq#99] Functions [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))] Aggregate Attributes [7]: [sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END))#297L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END))#298L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END))#299L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END))#300L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END))#301L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END))#302L, sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))#303L] Results [8]: [d_week_seq#99, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END))#297L,17,2) AS sun_sales#234, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END))#298L,17,2) AS mon_sales#235, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END))#299L,17,2) AS tue_sales#236, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END))#300L,17,2) AS wed_sales#237, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END))#301L,17,2) AS thu_sales#238, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END))#302L,17,2) AS fri_sales#239, MakeDecimal(sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))#303L,17,2) AS sat_sales#240] "
          },
          "3" : {
            "sign" : 683383451,
            "className" : "org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec",
            "sizeInBytes" : 5430494539148,
            "rowCount" : -1,
            "isRuntime" : false,
            "predicate" : " (unknown) ShuffleQueryStage Output [8]: [d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L] Arguments: 5 "
          }
        },
        "links" : [ {
          "fromId" : 3,
          "fromName" : "ShuffleQueryStage",
          "toId" : 2,
          "toName" : "HashAggregate",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "HashAggregate",
          "toId" : 1,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 4,
          "fromName" : "BroadcastQueryStage",
          "toId" : 1,
          "toName" : "BroadcastHashJoin",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "BroadcastHashJoin",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq#99 AS d_week_seq1#214, sun_sales#234 AS sun_sales1#215, mon_sales#235 AS mon_sales1#216, tue_sales#236 AS tue_sales1#217, wed_sales#237 AS wed_sales1#218, thu_sales#238 AS thu_sales1#219, fri_sales#239 AS fri_sales1#220, sat_sales#240 AS sat_sales1#221]\n+- BroadcastHashJoin [d_week_seq#99], [d_week_seq#245], Inner, BuildRight, false\n   :- HashAggregate(keys=[d_week_seq#99], functions=[sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sun_sales#234, mon_sales#235, tue_sales#236, wed_sales#237, thu_sales#238, fri_sales#239, sat_sales#240])\n   :  +- ShuffleQueryStage 5\n   :     +- Exchange hashpartitioning(d_week_seq#99, 200), ENSURE_REQUIREMENTS, [plan_id=859]\n   :        +- *(10) HashAggregate(keys=[d_week_seq#99], functions=[partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Sunday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Monday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Tuesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Wednesday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Thursday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Friday) THEN sales_price#231 END)), partial_sum(UnscaledValue(CASE WHEN (d_day_name#109 = Saturday) THEN sales_price#231 END))], output=[d_week_seq#99, sum#336L, sum#337L, sum#338L, sum#339L, sum#340L, sum#341L, sum#342L])\n   :           +- *(10) Project [sales_price#231, d_week_seq#99, d_day_name#109]\n   :              +- *(10) BroadcastHashJoin [sold_date_sk#230], [d_date_sk#95], Inner, BuildRight, false\n   :                 :- Union\n   :                 :  :- *(8) Project [ws_sold_date_sk#60 AS sold_date_sk#230, ws_ext_sales_price#49 AS sales_price#231]\n   :                 :  :  +- *(8) ColumnarToRow\n   :                 :  :     +- FileScan parquet spark_catalog.tpcds_100.web_sales[ws_ext_sales_price#49,ws_sold_date_sk#60] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1823 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/web_sa..., PartitionFilters: [isnotnull(ws_sold_date_sk#60)], PushedFilters: [], ReadSchema: struct<ws_ext_sales_price:decimal(7,2)>\n   :                 :  +- *(9) Project [cs_sold_date_sk#94 AS sold_date_sk#232, cs_ext_sales_price#83 AS sales_price#233]\n   :                 :     +- *(9) ColumnarToRow\n   :                 :        +- FileScan parquet spark_catalog.tpcds_100.catalog_sales[cs_ext_sales_price#83,cs_sold_date_sk#94] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1836 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/catalo..., PartitionFilters: [isnotnull(cs_sold_date_sk#94)], PushedFilters: [], ReadSchema: struct<cs_ext_sales_price:decimal(7,2)>\n   :                 +- BroadcastQueryStage 0\n   :                    +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=310]\n   :                       +- *(1) Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#323, [id=#136], xxhash64(d_week_seq#99, 42)))\n   :                          :  +- Subquery subquery#323, [id=#136]\n   :                          :     +- AdaptiveSparkPlan isFinalPlan=true\n                                       +- == Final Plan ==\n                                          ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                                          +- ShuffleQueryStage 0\n                                             +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=411]\n                                                +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                                   +- *(1) Project [d_week_seq#245]\n                                                      +- *(1) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                                         +- *(1) ColumnarToRow\n                                                            +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n                                       +- == Initial Plan ==\n                                          ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n                                          +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]\n                                             +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n                                                +- Project [d_week_seq#245]\n                                                   +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n                                                      +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n   :                          +- *(1) ColumnarToRow\n   :                             +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n   +- BroadcastQueryStage 1\n      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=332]\n         +- *(2) Project [d_week_seq#245]\n            +- *(2) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n               +- *(2) ColumnarToRow\n                  +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 1098032,
        "inputRowCount" : 1046
      },
      "InitialPartitionNum" : 200,
      "PD" : {
        "3" : [ 1032, 0, 1032, 960, 0, 0, 0, 0, 0, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 960, 960, 0, 0, 0, 0, 2664, 0, 0, 0, 0, 0, 0, 1920, 0, 960, 2064, 1032, 0, 0, 0, 0, 0, 0, 0, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 960, 0, 0, 0, 0, 0, 0, 0, 1032, 0, 0, 0, 0, 0, 0, 0, 1104, 0, 0, 0, 0, 1104, 0, 0, 0, 0, 1032, 0, 1104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1104, 960, 0, 0, 1032, 0, 960, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2056, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1032, 0, 0, 1104, 0, 0, 1032, 0, 0, 0, 0, 0, 1104, 0, 1104, 0, 825, 1032, 0, 1104, 0, 1032, 0, 0, 0, 0, 960, 0, 872, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2208, 0, 0, 0, 0, 0, 0, 1938, 1032, 952, 0, 0, 1104, 0, 0, 0, 1032, 0, 1104, 0, 0, 1032, 0, 0, 960, 0, 0, 1032, 0, 0, 0, 0, 1104, 0, 1032, 0, 0 ]
      },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 10,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 15 ],
      "Objectives" : {
        "DurationInMs" : 237,
        "TotalTasksDurationInMs" : 230,
        "IOBytes" : {
          "Total" : 51165,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 51165,
            "SW" : 0
          }
        }
      }
    },
    "6" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : -294912537,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 2629764,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 2629764,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_date_sk#95, d_week_seq#99, d_day_name#109] "
          },
          "1" : {
            "sign" : 331203717,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#326 [], xxhash64(d_week_seq#99, 42))) "
          },
          "2" : {
            "sign" : -513469665,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#95, d_date_id#96, d_date#97, d_month_seq#98, d_week_seq#99, d_quarter_seq#100, d_year#101, d_dow#102, d_moy#103, d_dom#104, d_qoy#105, d_fy_year#106, d_fy_quarter_seq#107, d_fy_week_seq#108, d_day_name#109, d_quarter_name#110, d_holiday#111, d_weekend#112, d_following_holiday#113, d_first_dom#114, d_last_dom#115, d_same_day_ly#116, d_same_day_lq#117, d_current_day#118, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "LogicalRelation",
          "toId" : 1,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Filter",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_date_sk#95, d_week_seq#99, d_day_name#109]\n+- Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#326 [], xxhash64(d_week_seq#99, 42)))\n   :  +- Aggregate [bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0) AS bloomFilter#325]\n   :     +- Project [d_week_seq#273]\n   :        +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n   :           +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#269,d_date_id#270,d_date#271,d_month_seq#272,d_week_seq#273,d_quarter_seq#274,d_year#275,d_dow#276,d_moy#277,d_dom#278,d_qoy#279,d_fy_year#280,d_fy_quarter_seq#281,d_fy_week_seq#282,d_day_name#283,d_quarter_name#284,d_holiday#285,d_weekend#286,d_following_holiday#287,d_first_dom#288,d_last_dom#289,d_same_day_ly#290,d_same_day_lq#291,d_current_day#292,... 4 more fields] parquet\n   +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_date_id#96,d_date#97,d_month_seq#98,d_week_seq#99,d_quarter_seq#100,d_year#101,d_dow#102,d_moy#103,d_dom#104,d_qoy#105,d_fy_year#106,d_fy_quarter_seq#107,d_fy_week_seq#108,d_day_name#109,d_quarter_name#110,d_holiday#111,d_weekend#112,d_following_holiday#113,d_first_dom#114,d_last_dom#115,d_same_day_ly#116,d_same_day_lq#117,d_current_day#118,... 4 more fields] parquet\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -1544302244,
            "className" : "org.apache.spark.sql.execution.FilterExec",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Input [3]: [d_date_sk#95, d_week_seq#99, d_day_name#109] Condition : ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#326, [id=#149], xxhash64(d_week_seq#99, 42))) "
          },
          "1" : {
            "sign" : 2020855523,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.date_dim Output [3]: [d_date_sk#95, d_week_seq#99, d_day_name#109] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim] PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)] ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string> "
          }
        },
        "links" : [ {
          "fromId" : 1,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.date_dim",
          "toId" : 0,
          "toName" : "Filter",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#326, [id=#149], xxhash64(d_week_seq#99, 42)))\n:  +- Subquery subquery#326, [id=#149]\n:     +- AdaptiveSparkPlan isFinalPlan=false\n:        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[bloomFilter#325])\n:           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=147]\n:              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#273, 42), 576, 14808, 0, 0)], output=[buf#358])\n:                 +- Project [d_week_seq#273]\n:                    +- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n:                       +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n+- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 2629764,
        "inputRowCount" : 73049
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 2,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 8 ],
      "Objectives" : {
        "DurationInMs" : 1302,
        "TotalTasksDurationInMs" : 1297,
        "IOBytes" : {
          "Total" : 427122,
          "Details" : {
            "IR" : 427122,
            "IW" : 0,
            "SR" : 0,
            "SW" : 0
          }
        }
      }
    },
    "1" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : 700534141,
            "className" : "org.apache.spark.sql.execution.adaptive.LogicalQueryStage",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 1888,
                "rowCount" : 1
              },
              "compileTime" : {
                "sizeInBytes" : 108,
                "rowCount" : 1
              }
            },
            "isRuntime" : true,
            "predicate" : " (unknown) LogicalQueryStage Arguments: Aggregate [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)]) "
          }
        },
        "links" : [ ],
        "rawPlan" : "LogicalQueryStage Aggregate [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322], ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)])\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -403210176,
            "className" : "org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec",
            "sizeInBytes" : 1888,
            "rowCount" : 1,
            "isRuntime" : true,
            "predicate" : " (unknown) ObjectHashAggregate Input [1]: [buf#357] Keys: [] Functions [1]: [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)] Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)#321] Results [1]: [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)#321 AS bloomFilter#322] "
          },
          "1" : {
            "sign" : 523180553,
            "className" : "org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec",
            "sizeInBytes" : 108,
            "rowCount" : 1,
            "isRuntime" : false,
            "predicate" : " (unknown) ShuffleQueryStage Output [1]: [buf#357] Arguments: 0 "
          }
        },
        "links" : [ {
          "fromId" : 1,
          "fromName" : "ShuffleQueryStage",
          "toId" : 0,
          "toName" : "ObjectHashAggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n+- ShuffleQueryStage 0\n   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=411]\n      +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n         +- *(1) Project [d_week_seq#245]\n            +- *(1) Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n               +- *(1) ColumnarToRow\n                  +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 1888,
        "inputRowCount" : 1
      },
      "InitialPartitionNum" : 1,
      "PD" : {
        "1" : [ 1538 ]
      },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 1,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 7,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 7 ],
      "Objectives" : {
        "DurationInMs" : 169,
        "TotalTasksDurationInMs" : 146,
        "IOBytes" : {
          "Total" : 1512,
          "Details" : {
            "IR" : 0,
            "IW" : 0,
            "SR" : 1512,
            "SW" : 0
          }
        }
      }
    },
    "0" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : -639620610,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Aggregate",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 108,
                "rowCount" : 1
              },
              "compileTime" : {
                "sizeInBytes" : 108,
                "rowCount" : 1
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Aggregate Arguments: [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322] "
          }
        },
        "links" : [ ],
        "rawPlan" : "Aggregate [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322]\n+- Project [d_week_seq#245]\n   +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n      +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#241,d_date_id#242,d_date#243,d_month_seq#244,d_week_seq#245,d_quarter_seq#246,d_year#247,d_dow#248,d_moy#249,d_dom#250,d_qoy#251,d_fy_year#252,d_fy_quarter_seq#253,d_fy_week_seq#254,d_day_name#255,d_quarter_name#256,d_holiday#257,d_weekend#258,d_following_holiday#259,d_first_dom#260,d_last_dom#261,d_same_day_ly#262,d_same_day_lq#263,d_current_day#264,... 4 more fields] parquet\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -734281258,
            "className" : "org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec",
            "sizeInBytes" : 108,
            "rowCount" : 1,
            "isRuntime" : false,
            "predicate" : " (unknown) ObjectHashAggregate Input [1]: [d_week_seq#245] Keys: [] Functions [1]: [partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)] Aggregate Attributes [1]: [buf#328] Results [1]: [buf#357] "
          },
          "1" : {
            "sign" : -2101464523,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [1]: [d_week_seq#245] Input [2]: [d_week_seq#245, d_year#247] "
          },
          "2" : {
            "sign" : -1378404129,
            "className" : "org.apache.spark.sql.execution.FilterExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Input [2]: [d_week_seq#245, d_year#247] Condition : ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245)) "
          },
          "3" : {
            "sign" : -1276366631,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.date_dim Output [2]: [d_week_seq#245, d_year#247] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim] PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)] ReadSchema: struct<d_week_seq:int,d_year:int> "
          }
        },
        "links" : [ {
          "fromId" : 3,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.date_dim",
          "toId" : 2,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 2,
          "fromName" : "Filter",
          "toId" : 1,
          "toName" : "Project",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Project",
          "toId" : 0,
          "toName" : "ObjectHashAggregate",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n+- Project [d_week_seq#245]\n   +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n      +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 6912,
        "inputRowCount" : 576
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 4,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 2 ],
      "Objectives" : {
        "DurationInMs" : 2318,
        "TotalTasksDurationInMs" : 2261,
        "IOBytes" : {
          "Total" : 84318,
          "Details" : {
            "IR" : 82828,
            "IW" : 0,
            "SR" : 0,
            "SW" : 1490
          }
        }
      }
    },
    "2" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : 1530053918,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 2629764,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 2629764,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_date_sk#95, d_week_seq#99, d_day_name#109] "
          },
          "1" : {
            "sign" : 1329242858,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#323 [], xxhash64(d_week_seq#99, 42))) "
          },
          "2" : {
            "sign" : -513469665,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#95, d_date_id#96, d_date#97, d_month_seq#98, d_week_seq#99, d_quarter_seq#100, d_year#101, d_dow#102, d_moy#103, d_dom#104, d_qoy#105, d_fy_year#106, d_fy_quarter_seq#107, d_fy_week_seq#108, d_day_name#109, d_quarter_name#110, d_holiday#111, d_weekend#112, d_following_holiday#113, d_first_dom#114, d_last_dom#115, d_same_day_ly#116, d_same_day_lq#117, d_current_day#118, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "LogicalRelation",
          "toId" : 1,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Filter",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_date_sk#95, d_week_seq#99, d_day_name#109]\n+- Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(scalar-subquery#323 [], xxhash64(d_week_seq#99, 42)))\n   :  +- Aggregate [bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0) AS bloomFilter#322]\n   :     +- Project [d_week_seq#245]\n   :        +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n   :           +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#241,d_date_id#242,d_date#243,d_month_seq#244,d_week_seq#245,d_quarter_seq#246,d_year#247,d_dow#248,d_moy#249,d_dom#250,d_qoy#251,d_fy_year#252,d_fy_quarter_seq#253,d_fy_week_seq#254,d_day_name#255,d_quarter_name#256,d_holiday#257,d_weekend#258,d_following_holiday#259,d_first_dom#260,d_last_dom#261,d_same_day_ly#262,d_same_day_lq#263,d_current_day#264,... 4 more fields] parquet\n   +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_date_id#96,d_date#97,d_month_seq#98,d_week_seq#99,d_quarter_seq#100,d_year#101,d_dow#102,d_moy#103,d_dom#104,d_qoy#105,d_fy_year#106,d_fy_quarter_seq#107,d_fy_week_seq#108,d_day_name#109,d_quarter_name#110,d_holiday#111,d_weekend#112,d_following_holiday#113,d_first_dom#114,d_last_dom#115,d_same_day_ly#116,d_same_day_lq#117,d_current_day#118,... 4 more fields] parquet\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -1945658184,
            "className" : "org.apache.spark.sql.execution.FilterExec",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Input [3]: [d_date_sk#95, d_week_seq#99, d_day_name#109] Condition : ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#323, [id=#136], xxhash64(d_week_seq#99, 42))) "
          },
          "1" : {
            "sign" : 2020855523,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 2629764,
            "rowCount" : 73049,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.date_dim Output [3]: [d_date_sk#95, d_week_seq#99, d_day_name#109] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim] PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)] ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string> "
          }
        },
        "links" : [ {
          "fromId" : 1,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.date_dim",
          "toId" : 0,
          "toName" : "Filter",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Filter ((isnotnull(d_date_sk#95) AND isnotnull(d_week_seq#99)) AND might_contain(Subquery subquery#323, [id=#136], xxhash64(d_week_seq#99, 42)))\n:  +- Subquery subquery#323, [id=#136]\n:     +- AdaptiveSparkPlan isFinalPlan=false\n:        +- ObjectHashAggregate(keys=[], functions=[bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[bloomFilter#322])\n:           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=134]\n:              +- ObjectHashAggregate(keys=[], functions=[partial_bloom_filter_agg(xxhash64(d_week_seq#245, 42), 576, 14808, 0, 0)], output=[buf#357])\n:                 +- Project [d_week_seq#245]\n:                    +- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n:                       +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n+- FileScan parquet spark_catalog.tpcds_100.date_dim[d_date_sk#95,d_week_seq#99,d_day_name#109] Batched: true, DataFilters: [isnotnull(d_date_sk#95), isnotnull(d_week_seq#99)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_date_sk), IsNotNull(d_week_seq)], ReadSchema: struct<d_date_sk:int,d_week_seq:int,d_day_name:string>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 2629764,
        "inputRowCount" : 73049
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 0,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 3 ],
      "Objectives" : {
        "DurationInMs" : 1962,
        "TotalTasksDurationInMs" : 1946,
        "IOBytes" : {
          "Total" : 84340,
          "Details" : {
            "IR" : 82828,
            "IW" : 0,
            "SR" : 0,
            "SW" : 1512
          }
        }
      }
    },
    "7" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : 249850892,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 6912,
                "rowCount" : 576
              },
              "compileTime" : {
                "sizeInBytes" : 6912,
                "rowCount" : 576
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#273] "
          },
          "1" : {
            "sign" : 109134331,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 141696,
                "rowCount" : 576
              },
              "compileTime" : {
                "sizeInBytes" : 141696,
                "rowCount" : 576
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273)) "
          },
          "2" : {
            "sign" : -1211842649,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#269, d_date_id#270, d_date#271, d_month_seq#272, d_week_seq#273, d_quarter_seq#274, d_year#275, d_dow#276, d_moy#277, d_dom#278, d_qoy#279, d_fy_year#280, d_fy_quarter_seq#281, d_fy_week_seq#282, d_day_name#283, d_quarter_name#284, d_holiday#285, d_weekend#286, d_following_holiday#287, d_first_dom#288, d_last_dom#289, d_same_day_ly#290, d_same_day_lq#291, d_current_day#292, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "LogicalRelation",
          "toId" : 1,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Filter",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq#273]\n+- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n   +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#269,d_date_id#270,d_date#271,d_month_seq#272,d_week_seq#273,d_quarter_seq#274,d_year#275,d_dow#276,d_moy#277,d_dom#278,d_qoy#279,d_fy_year#280,d_fy_quarter_seq#281,d_fy_week_seq#282,d_day_name#283,d_quarter_name#284,d_holiday#285,d_weekend#286,d_following_holiday#287,d_first_dom#288,d_last_dom#289,d_same_day_ly#290,d_same_day_lq#291,d_current_day#292,... 4 more fields] parquet\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : 1278028323,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [1]: [d_week_seq#273] Input [2]: [d_week_seq#273, d_year#275] "
          },
          "1" : {
            "sign" : 1227927444,
            "className" : "org.apache.spark.sql.execution.FilterExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Input [2]: [d_week_seq#273, d_year#275] Condition : ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273)) "
          },
          "2" : {
            "sign" : -1111050904,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.date_dim Output [2]: [d_week_seq#273, d_year#275] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim] PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)] ReadSchema: struct<d_week_seq:int,d_year:int> "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.date_dim",
          "toId" : 1,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Filter",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq#273]\n+- Filter ((isnotnull(d_year#275) AND (d_year#275 = 2002)) AND isnotnull(d_week_seq#273))\n   +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#273,d_year#275] Batched: true, DataFilters: [isnotnull(d_year#275), (d_year#275 = 2002), isnotnull(d_week_seq#273)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2002), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 6912,
        "inputRowCount" : 576
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 3,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 12 ],
      "Objectives" : {
        "DurationInMs" : 10088,
        "TotalTasksDurationInMs" : 88,
        "IOBytes" : {
          "Total" : 427122,
          "Details" : {
            "IR" : 427122,
            "IW" : 0,
            "SR" : 0,
            "SW" : 0
          }
        }
      }
    },
    "3" : {
      "QSLogical" : {
        "operators" : {
          "0" : {
            "sign" : 854548874,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Project",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 6912,
                "rowCount" : 576
              },
              "compileTime" : {
                "sizeInBytes" : 6912,
                "rowCount" : 576
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Project Arguments: [d_week_seq#245] "
          },
          "1" : {
            "sign" : -137345888,
            "className" : "org.apache.spark.sql.catalyst.plans.logical.Filter",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 141696,
                "rowCount" : 576
              },
              "compileTime" : {
                "sizeInBytes" : 141696,
                "rowCount" : 576
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Arguments: ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245)) "
          },
          "2" : {
            "sign" : -1792605082,
            "className" : "org.apache.spark.sql.execution.datasources.LogicalRelation",
            "stats" : {
              "runtime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              },
              "compileTime" : {
                "sizeInBytes" : 17970054,
                "rowCount" : 73049
              }
            },
            "isRuntime" : false,
            "predicate" : " (unknown) LogicalRelation Arguments: parquet, [d_date_sk#241, d_date_id#242, d_date#243, d_month_seq#244, d_week_seq#245, d_quarter_seq#246, d_year#247, d_dow#248, d_moy#249, d_dom#250, d_qoy#251, d_fy_year#252, d_fy_quarter_seq#253, d_fy_week_seq#254, d_day_name#255, d_quarter_name#256, d_holiday#257, d_weekend#258, d_following_holiday#259, d_first_dom#260, d_last_dom#261, d_same_day_ly#262, d_same_day_lq#263, d_current_day#264, ... 4 more fields], `spark_catalog`.`tpcds_100`.`date_dim`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, false "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "LogicalRelation",
          "toId" : 1,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Filter",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq#245]\n+- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n   +- Relation spark_catalog.tpcds_100.date_dim[d_date_sk#241,d_date_id#242,d_date#243,d_month_seq#244,d_week_seq#245,d_quarter_seq#246,d_year#247,d_dow#248,d_moy#249,d_dom#250,d_qoy#251,d_fy_year#252,d_fy_quarter_seq#253,d_fy_week_seq#254,d_day_name#255,d_quarter_name#256,d_holiday#257,d_weekend#258,d_following_holiday#259,d_first_dom#260,d_last_dom#261,d_same_day_ly#262,d_same_day_lq#263,d_current_day#264,... 4 more fields] parquet\n"
      },
      "QSPhysical" : {
        "operators" : {
          "0" : {
            "sign" : -2101464523,
            "className" : "org.apache.spark.sql.execution.ProjectExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Project Output [1]: [d_week_seq#245] Input [2]: [d_week_seq#245, d_year#247] "
          },
          "1" : {
            "sign" : -1378404129,
            "className" : "org.apache.spark.sql.execution.FilterExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Filter Input [2]: [d_week_seq#245, d_year#247] Condition : ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245)) "
          },
          "2" : {
            "sign" : -1276366631,
            "className" : "org.apache.spark.sql.execution.FileSourceScanExec",
            "sizeInBytes" : 6912,
            "rowCount" : 576,
            "isRuntime" : false,
            "predicate" : " (unknown) Scan parquet spark_catalog.tpcds_100.date_dim Output [2]: [d_week_seq#245, d_year#247] Batched: true Location: InMemoryFileIndex [hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim] PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)] ReadSchema: struct<d_week_seq:int,d_year:int> "
          }
        },
        "links" : [ {
          "fromId" : 2,
          "fromName" : "Scan parquet spark_catalog.tpcds_100.date_dim",
          "toId" : 1,
          "toName" : "Filter",
          "linkType" : "Operator"
        }, {
          "fromId" : 1,
          "fromName" : "Filter",
          "toId" : 0,
          "toName" : "Project",
          "linkType" : "Operator"
        } ],
        "rawPlan" : "Project [d_week_seq#245]\n+- Filter ((isnotnull(d_year#247) AND (d_year#247 = 2001)) AND isnotnull(d_week_seq#245))\n   +- FileScan parquet spark_catalog.tpcds_100.date_dim[d_week_seq#245,d_year#247] Batched: true, DataFilters: [isnotnull(d_year#247), (d_year#247 = 2001), isnotnull(d_week_seq#245)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://node7-opa:8020/user/spark_benchmark/tpcds_100/dataset/date_dim], PartitionFilters: [], PushedFilters: [IsNotNull(d_year), EqualTo(d_year,2001), IsNotNull(d_week_seq)], ReadSchema: struct<d_week_seq:int,d_year:int>\n"
      },
      "IM" : {
        "inputSizeInBytes" : 6912,
        "inputRowCount" : 576
      },
      "InitialPartitionNum" : 0,
      "PD" : { },
      "RunningQueryStageSnapshot" : {
        "RunningTasksNum" : 0,
        "FinishedTasksNum" : 0,
        "FinishedTasksTotalTimeInMs" : 0.0,
        "FinishedTasksDistributionInMs" : [ 0.0, 0.0, 0.0, 0.0, 0.0 ]
      },
      "QueryStageOptimizationId" : 1,
      "RuntimeConfiguration" : {
        "theta_p" : [ {
          "spark.sql.adaptive.advisoryPartitionSizeInBytes" : "64MB"
        }, {
          "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin" : "0.2"
        }, {
          "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold" : "0MB"
        }, {
          "spark.sql.adaptive.autoBroadcastJoinThreshold" : "10MB"
        }, {
          "spark.sql.shuffle.partitions" : "200"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes" : "256MB"
        }, {
          "spark.sql.adaptive.skewJoin.skewedPartitionFactor" : "5"
        }, {
          "spark.sql.files.maxPartitionBytes" : "128MB"
        }, {
          "spark.sql.files.openCostInBytes" : "4MB"
        } ],
        "theta_s" : [ {
          "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor" : "0.2"
        }, {
          "spark.sql.adaptive.coalescePartitions.minPartitionSize" : "1024KB"
        } ]
      },
      "RelevantQueryStageIds" : [ 4 ],
      "Objectives" : {
        "DurationInMs" : 1712,
        "TotalTasksDurationInMs" : 1707,
        "IOBytes" : {
          "Total" : 82828,
          "Details" : {
            "IR" : 82828,
            "IW" : 0,
            "SR" : 0,
            "SW" : 0
          }
        }
      }
    }
  },
  "SQLStartTimeInMs" : 1702226498999,
  "SQLEndTimeInMs" : 1702226519629,
  "Objectives" : {
    "DurationInMs" : 20630,
    "IOBytes" : {
      "Total" : 1831819723,
      "Details" : {
        "IR" : 1831540244,
        "IW" : 0,
        "SR" : 165341,
        "SW" : 114138
      }
    }
  }
}
