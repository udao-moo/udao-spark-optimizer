[
  {
    "id": "k1",
    "name": "spark.executor.cores",
    "type": "int",
    "construction_type": "int",
    "unit": null,
    "min": 1,
    "max": 5,
    "scale": "linear",
    "factor": 1,
    "base": null,
    "categories": null,
    "default": "1",
    "desc": "set among 1-5. Default: 1 for Yarn when k2=1"
  },
  {
    "id": "k2",
    "name": "spark.executor.memory",
    "type": "int",
    "construction_type": "int",
    "unit": "g",
    "min": 1,
    "max": 4,
    "scale": "linear",
    "factor": 1,
    "base": null,
    "categories": null,
    "default": "1g",
    "desc": "spark.executor.memory = k1 * k2, set memory among 1g-20g. Default: 1g when k1=1"
  },
  {
    "id": "k3",
    "name": "spark.executor.instances",
    "type": "int",
    "construction_type": "int",
    "unit": null,
    "min": 4,
    "max": 16,
    "scale": "linear",
    "factor": 1,
    "base": null,
    "categories": null,
    "default": "16",
    "desc": "set among 4-16, provide a total of 4-80 cores and 4-320g memory in executors. Default: 16 when k3=16, with 16 cores, and 16g memory"
  },
  {
    "id": "k4",
    "name": "spark.default.parallelism",
    "type": "int",
    "construction_type": "int",
    "unit": null,
    "min": 1,
    "max": 4,
    "scale": "linear",
    "factor": 1,
    "base": null,
    "categories": null,
    "default": "16",
    "desc": "spark.default.parallelism = k1 * k3 * k4, set among 4-320. Default: k4 = 1, parallel = 1 * 16 * 1 = 16"
  },
  {
    "id": "k5",
    "name": "spark.reducer.maxSizeInFlight",
    "type": "int",
    "construction_type": "int",
    "unit": "m",
    "min": 0,
    "max": 5,
    "scale": "log",
    "factor": 12,
    "base": 2,
    "categories": null,
    "default": "48m",
    "desc": "factor * (base ^ k5), set among 12m-384m. Default: 48m when k5 = 2"
  },
  {
    "id": "k6",
    "name": "spark.shuffle.sort.bypassMergeThreshold",
    "type": "bool",
    "construction_type": "int",
    "unit": null,
    "min": null,
    "max": null,
    "scale": null,
    "factor": null,
    "base": null,
    "categories": null,
    "default": "200",
    "desc": "set spark.shuffle.sort.bypassMergeThreshold < DOP when True, > DOP when False"
  },
  {
    "id": "k7",
    "name": "spark.shuffle.compress",
    "type": "bool",
    "construction_type": "bool",
    "unit": null,
    "min": null,
    "max": null,
    "scale": null,
    "factor": null,
    "base": null,
    "categories": null,
    "default": "true",
    "desc": "F/T, default: true when k7=1"
  },
  {
    "id": "k8",
    "name": "spark.memory.fraction",
    "type": "int",
    "construction_type": "float",
    "unit": null,
    "min": 50,
    "max": 75,
    "scale": "linear",
    "factor": 0.01,
    "base": null,
    "categories": null,
    "default": "0.6",
     "desc": "set among 0.50-0.75, default: 0.6 when k8=60, precision: 2"
  },
  {
    "id": "s1",
    "name": "spark.sql.adaptive.advisoryPartitionSizeInBytes",
    "type": "int",
    "construction_type": "int",
    "unit": "MB",
    "min": 0,
    "max": 5,
    "scale": "log",
    "factor": 16,
    "base": 2,
    "categories": null,
    "default": "64MB",
    "desc": "set among 16MB-512MB, default: 64MB when s1=2"
  },
  {
    "id": "s2",
    "name": "spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin",
    "type": "int",
    "construction_type": "float",
    "unit": null,
    "min": 1,
    "max": 6,
    "scale": "linear",
    "factor": 0.1,
    "base": null,
    "categories": null,
    "default": "0.2",
    "desc": "set among 0.1-0.6, default: 0.2 when s2=2"
  },
  {
    "id": "s3",
    "name": "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold",
    "type": "int",
    "unit": "MB",
    "construction_type": "int",
    "min": 0,
    "max": 32,
    "scale": "linear",
    "factor": 10,
    "base": null,
    "categories": null,
    "default": "0MB",
    "desc": "set among 0MB-320MB, default: 0MB when s3=0"
  },
  {
    "id": "s4",
    "name": "spark.sql.autoBroadcastJoinThreshold",
    "type": "int",
    "unit": "MB",
    "construction_type": "int",
    "min": 0,
    "max": 32,
    "scale": "linear",
    "factor": 10,
    "base": null,
    "categories": null,
    "default": "10MB",
    "desc": "set among 0MB-320MB, default: 10MB when s4=1"
  },
  {
    "id": "s5",
    "name": "spark.sql.shuffle.partitions",
    "type": "int",
    "unit": null,
    "construction_type": "int",
    "min": 2,
    "max": 50,
    "scale": "linear",
    "factor": 8,
    "base": null,
    "categories": null,
    "default": "200",
    "desc": "set among 16-400, default: 200 when s5=25"
  },
  {
    "id": "s6",
    "name": "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes",
    "type": "int",
    "unit": "MB",
    "construction_type": "int",
    "min": 0,
    "max": 4,
    "scale": "log",
    "factor": 64,
    "base": 2,
    "categories": null,
    "default": "256MB",
    "desc": "set among 64MB-1024MB, default: 256MB when s6=2"
  },
  {
    "id": "s7",
    "name": "spark.sql.adaptive.skewJoin.skewedPartitionFactor",
    "type": "int",
    "construction_type": "float",
    "unit": null,
    "min": 20,
    "max": 80,
    "scale": "linear",
    "factor": 0.1,
    "base": null,
    "categories": null,
    "default": "5",
    "desc": "set among 2.0-8.0, default: 5 when s7=50"
  },
  {
    "id": "s8",
    "name": "spark.sql.files.maxPartitionBytes",
    "type": "int",
    "construction_type": "int",
    "unit": "MB",
    "min": 0,
    "max": 4,
    "scale": "log",
    "factor": 32,
    "base": 2,
    "categories": null,
    "default": "128MB",
    "desc": "set among 32MB-512MB, default: 128MB when s8=2"
  },
  {
    "id": "s9",
    "name": "spark.sql.files.openCostInBytes",
    "type": "int",
    "construction_type": "int",
    "unit": "MB",
    "min": 0,
    "max": 4,
    "scale": "log",
    "factor": 1,
    "base": 2,
    "categories": null,
    "default": "4MB",
    "desc": "set among 1MB-16MB, default: 4MB when s9=2"
  },
  {
    "id": "s10",
    "name": "spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor",
    "type": "int",
    "construction_type": "float",
    "unit": null,
    "min": 5,
    "max": 35,
    "scale": "linear",
    "factor": 0.01,
    "base": null,
    "categories": null,
    "default": "0.2",
    "desc": "set among 0.05-0.35, default: 0.2 when s10=20"
  },
  {
    "id": "s11",
    "name": "spark.sql.adaptive.coalescePartitions.minPartitionSize",
    "type": "int",
    "construction_type": "int",
    "unit": "KB",
    "min": 1,
    "max": 6,
    "scale": "linear",
    "factor": 512,
    "base": null,
    "categories": null,
    "default": "1024KB",
    "desc": "set among 0.5-3 *1024KB, default: 1024KB(1MB) when s11=2"
  }
]
